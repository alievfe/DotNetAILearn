# https://zhuanlan.zhihu.com/p/712692839
{
  "data": {
    "requestId": "8b6db90c21bc0a8ab749c03b0a704b39",
    "autopromptString": "Heres a blog post about AI:",
    "resolvedSearchType": "neural",
    "results": [
      {
        "score": 0.1816248744726181,
        "title": "AI",
        "id": "https://blog.samaltman.com/ai",
        "url": "https://blog.samaltman.com/ai",
        "author": "",
        "text": "Yesterday at lunch a friend asked me what tech trend he should pay attention to but was probably ignoring. Without thinking much I said “artificial intelligence”, but having thought about that a bit more, I think it’s probably right. To be clear, AI (under the common scientific definition) likely won’t work. You can say that about any new technology, and it’s a generally correct statement. But I think most people are far too pessimistic about its chances - AI has not worked for so long that it’s acquired a bad reputation. CS professors mention it with a smirk. Neural networks failed the first time around, the logic goes, and so they won’t work this time either. But artificial general intelligence might work, and if it does, it will be the biggest development in technology ever. I’d argue we’ve gotten closer in lots of specific domains - for example, computers are now better than humans at lots of impressive things like playing chess and flying airplanes. But rather than call these examples of AIs, we just say that they weren’t really that hard in the first place. And to be fair, none of these really feel anything like a computer that can think like a human. There are a number of private (or recently acquired) companies, plus some large public ones, that are making impressive progress towards artificial general intelligence, but the good ones are very secretive about it. There are certainly some reasons to be optimistic. Andrew Ng, who worked or works on Google’s AI, has said that he believes learning comes from a single algorithm - the part of your brain that processes input from your ears is also capable of learning to process input from your eyes. If we can just figure out this one general-purpose algorithm, programs may be able to learn general-purpose things. There have been promising early results published from this sort of work, but because the brain is such a complex system so dependent on emergent behavior it’s difficult to say how close to the goal we really are. We understand how individual neurons work pretty well, and it’s possible that’s all we need to know to model how intelligence works. But the emergent behavior of 100 billion of them working together on the same principles gets extraordinarily complex, and difficult to model in software. Or, as Nick Sivo says, \"it's like reverse engineering the latest Intel processor with only the basic knowledge of how a transistor works.\" It’s also possible that there’s some other phenomenon responsible for intelligence, and the people working on this are on the wrong track. The biggest question for me is not about artificial intelligence, but instead about artificial consciousness, or creativity, or desire, or whatever you want to call it. I am quite confident that we’ll be able to make computer programs that perform specific complex tasks very well. But how do we make a computer program that decides what it wants to do? How do we make a computer decide to care on its own about learning to drive a car? Or write a novel? It’s possible--probable, even--that this sort of creativity will be an emergent property of learning in some non-intuitive way. Something happened in the course of evolution to make the human brain different from the reptile brain, which is closer to a computer that plays pong. (I originally was going to say a computer that plays chess, but computers play chess with no intuition or instinct--they just search a gigantic solution space very quickly.) And maybe we don't want to build machines that are concious in this sense. The most positive outcome I can think of is one where computers get really good at doing, and humans get really good at thinking. If we never figure out how to make computers creative, then there will be a very natural division of labor between man and machine.",
        "favicon": "https://phthemes.s3.amazonaws.com/189/ocI2l2NFgWKLlp1H/images/favicon.ico?v=1496356566"
      },
      {
        "score": 0.1779109090566635,
        "title": "The AI Revolution: The Road to Superintelligence",
        "id": "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html",
        "url": "https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html",
        "publishedDate": "2015-01-22T14:21:52.000Z",
        "author": "Tim Urban",
        "text": "PDF: We made a fancy PDF of this post for printing and offline viewing. Buy it here. (Or see a preview .) \n Note: The reason this post took three weeks to finish is that as I dug into research on Artificial Intelligence, I could not believe what I was reading. It hit me pretty quickly that what’s happening in the world of AI is not just an important topic, but by far THE most important topic for our future. So I wanted to learn as much as I could about it, and once I did that, I wanted to make sure I wrote a post that really explained this whole situation and why it matters so much. Not shockingly, that became outrageously long, so I broke it into two parts. This is Part 1—Part 2 is here . \n _______________ \n We are on the edge of change comparable to the rise of human life on Earth. — Vernor Vinge \n What does it feel like to stand here? \n \n It seems like a pretty intense place to be standing—but then you have to remember something about what it’s like to stand on a time graph: you can’t see what’s to your right. So here’s how it actually feels to stand there: \n \n Which probably feels pretty normal… \n _______________ \n The Far Future—Coming Soon \n Imagine taking a time machine back to 1750—a time when the world was in a permanent power outage, long-distance communication meant either yelling loudly or firing a cannon in the air, and all transportation ran on hay. When you get there, you retrieve a dude, bring him to 2015, and then walk him around and watch him react to everything. It’s impossible for us to understand what it would be like for him to see shiny capsules racing by on a highway, talk to people who had been on the other side of the ocean earlier in the day, watch sports that were being played 1,000 miles away, hear a musical performance that happened 50 years ago, and play with my magical wizard rectangle that he could use to capture a real-life image or record a living moment, generate a map with a paranormal moving blue dot that shows him where he is, look at someone’s face and chat with them even though they’re on the other side of the country, and worlds of other inconceivable sorcery. This is all before you show him the internet or explain things like the International Space Station, the Large Hadron Collider, nuclear weapons, or general relativity. \n This experience for him wouldn’t be surprising or shocking or even mind-blowing—those words aren’t big enough. He might actually die. \n But here’s the interesting thing—if he then went back to 1750 and got jealous that we got to see his reaction and decided he wanted to try the same thing, he’d take the time machine and go back the same distance, get someone from around the year 1500, bring him to 1750, and show him everything. And the 1500 guy would be shocked by a lot of things—but he wouldn’t die. It would be far less of an insane experience for him, because while 1500 and 1750 were very different, they were much less different than 1750 to 2015. The 1500 guy would learn some mind-bending shit about space and physics, he’d be impressed with how committed Europe turned out to be with that new imperialism fad, and he’d have to do some major revisions of his world map conception. But watching everyday life go by in 1750—transportation, communication, etc.—definitely wouldn’t make him die. \n No, in order for the 1750 guy to have as much fun as we had with him, he’d have to go much farther back—maybe all the way back to about 12,000 BC, before the First Agricultural Revolution gave rise to the first cities and to the concept of civilization. If someone from a purely hunter-gatherer world—from a time when humans were, more or less, just another animal species—saw the vast human empires of 1750 with their towering churches, their ocean-crossing ships, their concept of being “inside,” and their enormous mountain of collective, accumulated human knowledge and discovery—he’d likely die. \n And then what if, after dying, he got jealous and wanted to do the same thing. If he went back 12,000 years to 24,000 BC and got a guy and brought him to 12,000 BC, he’d show the guy everything and the guy would be like, “Okay what’s your point who cares.” For the 12,000 BC guy to have the same fun, he’d have to go back over 100,000 years and get someone he could show fire and language to for the first time. \n In order for someone to be transported into the future and die from the level of shock they’d experience, they have to go enough years ahead that a “die level of progress,” or a Die Progress Unit (DPU) has been achieved. So a DPU took over 100,000 years in hunter-gatherer times, but at the post-Agricultural Revolution rate, it only took about 12,000 years. The post-Industrial Revolution world has moved so quickly that a 1750 person only needs to go forward a couple hundred years for a DPU to have happened. \n This pattern—human progress moving quicker and quicker as time goes on—is what futurist Ray Kurzweil calls human history’s Law of Accelerating Returns. This happens because more advanced societies have the ability to progress at a faster rate than less advanced societies— because they’re more advanced. 19th century humanity knew more and had better technology than 15th century humanity, so it’s no surprise that humanity made far more advances in the 19th century than in the 15th century—15th century humanity was no match for 19th century humanity. 1 1 ← open these \n This works on smaller scales too. The movie Back to the Future came out in 1985, and “the past” took place in 1955. In the movie, when Michael J. Fox went back to 1955, he was caught off-guard by the newness of TVs, the prices of soda, the lack of love for shrill electric guitar, and the variation in slang. It was a different world, yes—but if the movie were made today and the past took place in 1985, the movie could have had much more fun with much bigger differences. The character would be in a time before personal computers, internet, or cell phones—today’s Marty McFly, a teenager born in the late 90s, would be much more out of place in 1985 than the movie’s Marty McFly was in 1955. \n This is for the same reason we just discussed—the Law of Accelerating Returns. The average rate of advancement between 1985 and 2015 was higher than the rate between 1955 and 1985—because the former was a more advanced world—so much more change happened in the most recent 30 years than in the prior 30. \n So—advances are getting bigger and bigger and happening more and more quickly. This suggests some pretty intense things about our future, right? \n Kurzweil suggests that the progress of the entire 20th century would have been achieved in only 20 years at the rate of advancement in the year 2000—in other words, by 2000, the rate of progress was five times faster than the average rate of progress during the 20th century. He believes another 20th century’s worth of progress happened between 2000 and 2014 and that another 20th century’s worth of progress will happen by 2021, in only seven years. A couple decades later, he believes a 20th century’s worth of progress will happen multiple times in the same year, and even later, in less than one month. All in all, because of the Law of Accelerating Returns, Kurzweil believes that the 21st century will achieve 1,000 times the progress of the 20th century. 2 \n If Kurzweil and others who agree with him are correct, then we may be as blown away by 2030 as our 1750 guy was by 2015—i.e. the next DPU might only take a couple decades—and the world in 2050 might be so vastly different than today’s world that we would barely recognize it. \n This isn’t science fiction. It’s what many scientists smarter and more knowledgeable than you or I firmly believe—and if you look at history, it’s what we should logically predict. \n So then why, when you hear me say something like “the world 35 years from now might be totally unrecognizable,” are you thinking, “Cool….but nahhhhhhh”? Three reasons we’re skeptical of outlandish forecasts of the future: \n 1) When it comes to history, we think in straight lines. When we imagine the progress of the next 30 years, we look back to the progress of the previous 30 as an indicator of how much will likely happen. When we think about the extent to which the world will change in the 21st century, we just take the 20th century progress and add it to the year 2000. This was the same mistake our 1750 guy made when he got someone from 1500 and expected to blow his mind as much as his own was blown going the same distance ahead. It’s most intuitive for us to think linearly, when we should be thinking exponentially . If someone is being more clever about it, they might predict the advances of the next 30 years not by looking at the previous 30 years, but by taking the current rate of progress and judging based on that. They’d be more accurate, but still way off. In order to think about the future correctly, you need to imagine things moving at a much faster rate than they’re moving now. \n \n 2) The trajectory of very recent history often tells a distorted story. First, even a steep exponential curve seems linear when you only look at a tiny slice of it, the same way if you look at a little segment of a huge circle up close, it looks almost like a straight line. Second, exponential growth isn’t totally smooth and uniform. Kurzweil explains that progress happens in “S-curves”: \n \n An S is created by the wave of progress when a new paradigm sweeps the world. The curve goes through three phases: \n 1. Slow growth (the early phase of exponential growth) 2. Rapid growth (the late, explosive phase of exponential growth) 3. A leveling off as the particular paradigm matures 3 \n If you look only at very recent history, the part of the S-curve you’re on at the moment can obscure your perception of how fast things are advancing. The chunk of time between 1995 and 2007 saw the explosion of the internet, the introduction of Microsoft, Google, and Facebook into the public consciousness, the birth of social networking, and the introduction of cell phones and then smart phones. That was Phase 2: the growth spurt part of the S. But 2008 to 2015 has been less groundbreaking, at least on the technological front. Someone thinking about the future today might examine the last few years to gauge the current rate of advancement, but that’s missing the bigger picture. In fact, a new, huge Phase 2 growth spurt might be brewing right now. \n 3) Our own experience makes us stubborn old men about the future. We base our ideas about the world on our personal experience, and that experience has ingrained the rate of growth of the recent past in our heads as “the way things happen.” We’re also limited by our imagination, which takes our experience and uses it to conjure future predictions—but often, what we know simply doesn’t give us the tools to think accurately about the future. 2 When we hear a prediction about the future that contradicts our experience-based notion of how things work , our instinct is that the prediction must be naive. If I tell you, later in this post, that you may live to be 150, or 250, or not die at all , your instinct will be, “That’s stupid—if there’s one thing I know from history, it’s that everybody dies.” And yes, no one in the past has not died. But no one flew airplanes before airplanes were invented either. \n So while nahhhhh might feel right as you read this post, it’s probably actually wrong. The fact is, if we’re being truly logical and expecting historical patterns to continue, we should conclude that much, much, much more should change in the coming decades than we intuitively expect. Logic also suggests that if the most advanced species on a planet keeps making larger and larger leaps forward at an ever-faster rate, at some point, they’ll make a leap so great that it completely alters life as they know it and the perception they have of what it means to be a human—kind of like how evolution kept making great leaps toward intelligence until finally it made such a large leap to the human being that it completely altered what it meant for any creature to live on planet Earth. And if you spend some time reading about what’s going on today in science and technology, you start to see a lot of signs quietly hinting that life as we currently know it cannot withstand the leap that’s coming next. \n _______________ \n What Is AI? \n If you’re like me, you used to think Artificial Intelligence was a silly sci-fi concept, but lately you’ve been hearing it mentioned by serious people, and you don’t really quite get it. \n There are three reasons a lot of people are confused about the term AI: \n 1) We associate AI with movies. Star Wars. Terminator. 2001: A Space Odyssey. Even the Jetsons. And those are fiction, as are the robot characters. So it makes AI sound a little fictional to us. \n 2) AI is a broad topic. It ranges from your phone’s calculator to self-driving cars to something in the future that might change the world dramatically. AI refers to all of these things, which is confusing. \n 3) We use AI all the time in our daily lives, but we often don’t realize it’s AI. John McCarthy, who coined the term “Artificial Intelligence” in 1956, complained that “as soon as it works, no one calls it AI anymore.” 4 Because of this phenomenon, AI often sounds like a mythical future prediction more than a reality. At the same time, it makes it sound like a pop concept from the past that never came to fruition. Ray Kurzweil says he hears people say that AI withered in the 1980s, which he compares to “insisting that the Internet died in the dot-com bust of the early 2000s.” 5 \n So let’s clear things up. First, stop thinking of robots . A robot is a container for AI, sometimes mimicking the human form, sometimes not—but the AI itself is the computer inside the robot. AI is the brain, and the robot is its body—if it even has a body. For example, the software and data behind Siri is AI, the woman’s voice we hear is a personification of that AI, and there’s no robot involved at all. \n Secondly, you’ve probably heard the term “singularity” or “technological singularity.” This term has been used in math to describe an asymptote-like situation where normal rules no longer apply. It’s been used in physics to describe a phenomenon like an infinitely small, dense black hole or the point we were all squished into right before the Big Bang. Again, situations where the usual rules don’t apply. In 1993, Vernor Vinge wrote a famous essay in which he applied the term to the moment in the future when our technology’s intelligence exceeds our own—a moment for him when life as we know it will be forever changed and normal rules will no longer apply. Ray Kurzweil then muddled things a bit by defining the singularity as the time when the Law of Accelerating Returns has reached such an extreme pace that technological progress is happening at a seemingly-infinite pace, and after which we’ll be living in a whole new world. I found that many of today’s AI thinkers have stopped using the term, and it’s confusing anyway, so I won’t use it much here (even though we’ll be focusing on that idea throughout). \n Finally, while there are many different types or forms of AI since AI is a broad concept, the critical categories we need to think about are based on an AI’s caliber . There are three major AI caliber categories: \n AI Caliber 1) Artificial Narrow Intelligence (ANI): Sometimes referred to as Weak AI , Artificial Narrow Intelligence is AI that specializes in one area. There’s AI that can beat the world chess champion in chess, but that’s the only thing it does. Ask it to figure out a better way to store data on a hard drive, and it’ll look at you blankly. \n AI Caliber 2) Artificial General Intelligence (AGI): Sometimes referred to as Strong AI , or Human-Level AI , Artificial General Intelligence refers to a computer that is as smart as a human across the board— a machine that can perform any intellectual task that a human being can. Creating AGI is a much harder task than creating ANI, and we’re yet to do it. Professor Linda Gottfredson describes intelligence as “a very general mental capability that, among other things, involves the ability to reason, plan, solve problems, think abstractly, comprehend complex ideas, learn quickly, and learn from experience.” AGI would be able to do all of those things as easily as you can. \n AI Caliber 3) Artificial Superintelligence (ASI): Oxford philosopher and leading AI thinker Nick Bostrom defines superintelligence as “an intellect that is much smarter than the best human brains in practically every field, including scientific creativity, general wisdom and social skills.” Artificial Superintelligence ranges from a computer that’s just a little smarter than a human to one that’s trillions of times smarter—across the board. ASI is the reason the topic of AI is such a spicy meatball and why the words “immortality” and “extinction” will both appear in these posts multiple times. \n As of now, humans have conquered the lowest caliber of AI—ANI—in many ways, and it’s everywhere. The AI Revolution is the road from ANI, through AGI, to ASI—a road we may or may not survive but that, either way, will change everything. \n Let’s take a close look at what the leading thinkers in the field believe this road looks like and why this revolution might happen way sooner than you might think: \n Where We Are Currently—A World Running on ANI \n Artificial Narrow Intelligence is machine intelligence that equals or exceeds human intelligence or efficiency at a specific thing. A few examples: \n \n Cars are full of ANI systems, from the computer that figures out when the anti-lock brakes should kick in to the computer that tunes the parameters of the fuel injection systems. Google’s self-driving car , which is being tested now, will contain robust ANI systems that allow it to perceive and react to the world around it. \n Your phone is a little ANI factory. When you navigate using your map app, receive tailored music recommendations from Pandora, check tomorrow’s weather, talk to Siri, or dozens of other everyday activities, you’re using ANI. \n Your email spam filter is a classic type of ANI—it starts off loaded with intelligence about how to figure out what’s spam and what’s not, and then it learns and tailors its intelligence to you as it gets experience with your particular preferences. The Nest Thermostat does the same thing as it starts to figure out your typical routine and act accordingly. \n You know the whole creepy thing that goes on when you search for a product on Amazon and then you see that as a “recommended for you” product on a different site, or when Facebook somehow knows who it makes sense for you to add as a friend? That’s a network of ANI systems, working together to inform each other about who you are and what you like and then using that information to decide what to show you. Same goes for Amazon’s “People who bought this also bought…” thing—that’s an ANI system whose job it is to gather info from the behavior of millions of customers and synthesize that info to cleverly upsell you so you’ll buy more things. \n Google Translate is another classic ANI system—impressively good at one narrow task. Voice recognition is another, and there are a bunch of apps that use those two ANIs as a tag team, allowing you to speak a sentence in one language and have the phone spit out the same sentence in another. \n When your plane lands, it’s not a human that decides which gate it should go to. Just like it’s not a human that determined the price of your ticket. \n The world’s best Checkers, Chess, Scrabble, Backgammon, and Othello players are now all ANI systems. \n Google search is one large ANI brain with incredibly sophisticated methods for ranking pages and figuring out what to show you in particular. Same goes for Facebook’s Newsfeed. \n And those are just in the consumer world. Sophisticated ANI systems are widely used in sectors and industries like military, manufacturing, and finance (algorithmic high-frequency AI traders account for more than half of equity shares traded on US markets 6 ), and in expert systems like those that help doctors make diagnoses and, most famously, IBM’s Watson , who contained enough facts and understood coy Trebek-speak well enough to soundly beat the most prolific Jeopardy champions. \n \n ANI systems as they are now aren’t especially scary. At worst, a glitchy or badly-programmed ANI can cause an isolated catastrophe like knocking out a power grid, causing a harmful nuclear power plant malfunction, or triggering a financial markets disaster (like the 2010 Flash Crash when an ANI program reacted the wrong way to an unexpected situation and caused the stock market to briefly plummet, taking $1 trillion of market value with it, only part of which was recovered when the mistake was corrected). \n But while ANI doesn’t have the capability to cause an existential threat , we should see this increasingly large and complex ecosystem of relatively-harmless ANI as a precursor of the world-altering hurricane that’s on the way. Each new ANI innovation quietly adds another brick onto the road to AGI and ASI. Or as Aaron Saenz sees it , our world’s ANI systems “are like the amino acids in the early Earth’s primordial ooze”—the inanimate stuff of life that, one unexpected day, woke up. \n The Road From ANI to AGI \n Why It’s So Hard \n Nothing will make you appreciate human intelligence like learning about how unbelievably challenging it is to try to create a computer as smart as we are. Building skyscrapers, putting humans in space, figuring out the details of how the Big Bang went down—all far easier than understanding our own brain or how to make something as cool as it. As of now, the human brain is the most complex object in the known universe. \n What’s interesting is that the hard parts of trying to build AGI (a computer as smart as humans in general , not just at one narrow specialty) are not intuitively what you’d think they are. Build a computer that can multiply two ten-digit numbers in a split second—incredibly easy. Build one that can look at a dog and answer whether it’s a dog or a cat—spectacularly difficult. Make AI that can beat any human in chess? Done. Make one that can read a paragraph from a six-year-old’s picture book and not just recognize the words but understand the meaning of them? Google is currently spending billions of dollars trying to do it. Hard things—like calculus, financial market strategy, and language translation—are mind-numbingly easy for a computer, while easy things—like vision, motion, movement, and perception—are insanely hard for it. Or, as computer scientist Donald Knuth puts it, “AI has by now succeeded in doing essentially everything that requires ‘thinking’ but has failed to do most of what people and animals do ‘without thinking.'” 7 \n What you quickly realize when you think about this is that those things that seem easy to us are actually unbelievably complicated, and they only seem easy because those skills have been optimized in us (and most animals) by hundreds of millions of years of animal evolution. When you reach your hand up toward an object, the muscles, tendons, and bones in your shoulder, elbow, and wrist instantly perform a long series of physics operations, in conjunction with your eyes, to allow you to move your hand in a straight line through three dimensions. It seems effortless to you because you have perfected software in your brain for doing it. Same idea goes for why it’s not that malware is dumb for not being able to figure out the slanty word recognition test when you sign up for a new account on a site—it’s that your brain is super impressive for being able to. \n On the other hand, multiplying big numbers or playing chess are new activities for biological creatures and we haven’t had any time to evolve a proficiency at them, so a computer doesn’t need to work too hard to beat us. Think about it—which would you rather do, build a program that could multiply big numbers or one that could understand the essence of a B well enough that you could show it a B in any one of thousands of unpredictable fonts or handwriting and it could instantly know it was a B? \n One fun example—when you look at this, you and a computer both can figure out that it’s a rectangle with two distinct shades, alternating: \n \n Tied so far. But if you pick up the black and reveal the whole image… \n \n …you have no problem giving a full description of the various opaque and translucent cylinders, slats, and 3-D corners, but the computer would fail miserably. It would describe what it sees—a variety of two-dimensional shapes in several different shades—which is actually what’s there. Your brain is doing a ton of fancy shit to interpret the implied depth, shade-mixing, and room lighting the picture is trying to portray. 8 And looking at the picture below, a computer sees a two-dimensional white, black, and gray collage, while you easily see what it really is—a photo of an entirely-black, 3-D rock: \n Credit: Matthew Lloyd \n And everything we just mentioned is still only taking in stagnant information and processing it. To be human-level intelligent, a computer would have to understand things like the difference between subtle facial expressions, the distinction between being pleased, relieved, content, satisfied, and glad, and why Braveheart was great but The Patriot was terrible. \n Daunting. \n So how do we get there? \n First Key to Creating AGI: Increasing Computational Power \n One thing that definitely needs to happen for AGI to be a possibility is an increase in the power of computer hardware. If an AI system is going to be as intelligent as the brain, it’ll need to equal the brain’s raw computing capacity. \n One way to express this capacity is in the total calculations per second (cps) the brain could manage, and you could come to this number by figuring out the maximum cps of each structure in the brain and then adding them all together. \n Ray Kurzweil came up with a shortcut by taking someone’s professional estimate for the cps of one structure and that structure’s weight compared to that of the whole brain and then multiplying proportionally to get an estimate for the total. Sounds a little iffy, but he did this a bunch of times with various professional estimates of different regions, and the total always arrived in the same ballpark—around 10 16 , or 10 quadrillion cps. \n Currently, the world’s fastest supercomputer, China’s Tianhe-2 , has actually beaten that number, clocking in at about 34 quadrillion cps. But Tianhe-2 is also a dick, taking up 720 square meters of space, using 24 megawatts of power (the brain runs on just 20 watts ), and costing $390 million to build. Not especially applicable to wide usage, or even most commercial or industrial usage yet. \n Kurzweil suggests that we think about the state of computers by looking at how many cps you can buy for $1,000. When that number reaches human-level—10 quadrillion cps—then that’ll mean AGI could become a very real part of life. \n Moore’s Law is a historically-reliable rule that the world’s maximum computing power doubles approximately every two years, meaning computer hardware advancement, like general human advancement through history, grows exponentially. Looking at how this relates to Kurzweil’s cps/$1,000 metric, we’re currently at about 10 trillion cps/$1,000, right on pace with this graph’s predicted trajectory: 9 \n \n So the world’s $1,000 computers are now beating the mouse brain and they’re at about a thousandth of human level. This doesn’t sound like much until you remember that we were at about a trillionth of human level in 1985, a billionth in 1995, and a millionth in 2005. Being at a thousandth in 2015 puts us right on pace to get to an affordable computer by 2025 that rivals the power of the brain. \n So on the hardware side, the raw power needed for AGI is technically available now, in China, and we’ll be ready for affordable, widespread AGI-caliber hardware within 10 years. But raw computational power alone doesn’t make a computer generally intelligent—the next question is, how do we bring human-level intelligence to all that power? \n Second Key to Creating AGI: Making It Smart \n This is the icky part. The truth is, no one really knows how to make it smart—we’re still debating how to make a computer human-level intelligent and capable of knowing what a dog and a weird-written B and a mediocre movie is. But there are a bunch of far-fetched strategies out there and at some point, one of them will work. Here are the three most common strategies I came across: \n 1) Plagiarize the brain. \n This is like scientists toiling over how that kid who sits next to them in class is so smart and keeps doing so well on the tests, and even though they keep studying diligently, they can’t do nearly as well as that kid, and then they finally decide “k fuck it I’m just gonna copy that kid’s answers.” It makes sense—we’re stumped trying to build a super-complex computer, and there happens to be a perfect prototype for one in each of our heads. \n The science world is working hard on reverse engineering the brain to figure out how evolution made such a rad thing— optimistic estimates say we can do this by 2030. Once we do that, we’ll know all the secrets of how the brain runs so powerfully and efficiently and we can draw inspiration from it and steal its innovations. One example of computer architecture that mimics the brain is the artificial neural network. It starts out as a network of transistor “neurons,” connected to each other with inputs and outputs, and it knows nothing—like an infant brain. The way it “learns” is it tries to do a task, say handwriting recognition, and at first, its neural firings and subsequent guesses at deciphering each letter will be completely random. But when it’s told it got something right, the transistor connections in the firing pathways that happened to create that answer are strengthened; when it’s told it was wrong, those pathways’ connections are weakened. After a lot of this trial and feedback, the network has, by itself, formed smart neural pathways and the machine has become optimized for the task. The brain learns a bit like this but in a more sophisticated way, and as we continue to study the brain, we’re discovering ingenious new ways to take advantage of neural circuitry. \n More extreme plagiarism involves a strategy called “whole brain emulation,” where the goal is to slice a real brain into thin layers, scan each one, use software to assemble an accurate reconstructed 3-D model, and then implement the model on a powerful computer. We’d then have a computer officially capable of everything the brain is capable of—it would just need to learn and gather information. If engineers get really good, they’d be able to emulate a real brain with such exact accuracy that the brain’s full personality and memory would be intact once the brain architecture has been uploaded to a computer. If the brain belonged to Jim right before he passed away, the computer would now wake up as Jim ( ? ), which would be a robust human-level AGI, and we could now work on turning Jim into an unimaginably smart ASI, which he’d probably be really excited about. \n How far are we from achieving whole brain emulation? Well so far, we’ve not yet just recently been able to emulate a 1mm-long flatworm brain, which consists of just 302 total neurons. The human brain contains 100 billion. If that makes it seem like a hopeless project, remember the power of exponential progress—now that we’ve conquered the tiny worm brain, an ant might happen before too long, followed by a mouse, and suddenly this will seem much more plausible. \n 2) Try to make evolution do what it did before but for us this time. \n So if we decide the smart kid’s test is too hard to copy, we can try to copy the way he studies for the tests instead. \n Here’s something we know. Building a computer as powerful as the brain is possible—our own brain’s evolution is proof. And if the brain is just too complex for us to emulate, we could try to emulate evolution instead. The fact is, even if we can emulate a brain, that might be like trying to build an airplane by copying a bird’s wing-flapping motions—often, machines are best designed using a fresh, machine-oriented approach, not by mimicking biology exactly. \n So how can we simulate evolution to build AGI? The method, called “genetic algorithms,” would work something like this: there would be a performance-and-evaluation process that would happen again and again (the same way biological creatures “perform” by living life and are “evaluated” by whether they manage to reproduce or not). A group of computers would try to do tasks, and the most successful ones would be bred with each other by having half of each of their programming merged together into a new computer. The less successful ones would be eliminated. Over many, many iterations, this natural selection process would produce better and better computers. The challenge would be creating an automated evaluation and breeding cycle so this evolution process could run on its own. \n The downside of copying evolution is that evolution likes to take a billion years to do things and we want to do this in a few decades. \n But we have a lot of advantages over evolution. First, evolution has no foresight and works randomly—it produces more unhelpful mutations than helpful ones, but we would control the process so it would only be driven by beneficial glitches and targeted tweaks. Secondly, evolution doesn’t aim for anything, including intelligence—sometimes an environment might even select against higher intelligence (since it uses a lot of energy). We, on the other hand, could specifically direct this evolutionary process toward increasing intelligence. Third, to select for intelligence, evolution has to innovate in a bunch of other ways to facilitate intelligence—like revamping the ways cells produce energy—when we can remove those extra burdens and use things like electricity. It’s no doubt we’d be much, much faster than evolution—but it’s still not clear whether we’ll be able to improve upon evolution enough to make this a viable strategy. \n 3) Make this whole thing the computer’s problem, not ours. \n This is when scientists get desperate and try to program the test to take itself. But it might be the most promising method we have. \n The idea is that we’d build a computer whose two major skills would be doing research on AI and coding changes into itself—allowing it to not only learn but to improve its own architecture . We’d teach computers to be computer scientists so they could bootstrap their own development. And that would be their main job—figuring out how to make themselves smarter. More on this later. \n All of This Could Happen Soon \n Rapid advancements in hardware and innovative experimentation with software are happening simultaneously, and AGI could creep up on us quickly and unexpectedly for two main reasons: \n 1) Exponential growth is intense and what seems like a snail’s pace of advancement can quickly race upwards—this GIF illustrates this concept nicely: \n 2) When it comes to software, progress can seem slow, but then one epiphany can instantly change the rate of advancement (kind of like the way science, during the time humans thought the universe was geocentric, was having difficulty calculating how the universe worked, but then the discovery that it was heliocentric suddenly made everything much easier). Or, when it comes to something like a computer that improves itself, we might seem far away but actually be just one tweak of the system away from having it become 1,000 times more effective and zooming upward to human-level intelligence. \n The Road From AGI to ASI \n At some point, we’ll have achieved AGI—computers with human-level general intelligence. Just a bunch of people and computers living together in equality. \n Oh actually not at all. \n The thing is, AGI with an identical level of intelligence and computational capacity as a human would still have significant advantages over humans. Like: \n Hardware: \n \n Speed. The brain’s neurons max out at around 200 Hz, while today’s microprocessors (which are much slower than they will be when we reach AGI) run at 2 GHz, or 10 million times faster than our neurons. And the brain’s internal communications, which can move at about 120 m/s, are horribly outmatched by a computer’s ability to communicate optically at the speed of light. \n Size and storage. The brain is locked into its size by the shape of our skulls, and it couldn’t get much bigger anyway, or the 120 m/s internal communications would take too long to get from one brain structure to another. Computers can expand to any physical size, allowing far more hardware to be put to work, a much larger working memory (RAM), and a longterm memory (hard drive storage) that has both far greater capacity and precision than our own. \n Reliability and durability. It’s not only the memories of a computer that would be more precise. Computer transistors are more accurate than biological neurons, and they’re less likely to deteriorate (and can be repaired or replaced if they do). Human brains also get fatigued easily, while computers can run nonstop, at peak performance, 24/7. \n \n Software: \n \n Editability, upgradability, and a wider breadth of possibility. Unlike the human brain, computer software can receive updates and fixes and can be easily experimented on. The upgrades could also span to areas where human brains are weak. Human vision software is superbly advanced, while its complex engineering capability is pretty low-grade. Computers could match the human on vision software but could also become equally optimized in engineering and any other area. \n Collective capability. Humans crush all other species at building a vast collective intelligence. Beginning with the development of language and the forming of large, dense communities, advancing through the inventions of writing and printing, and now intensified through tools like the internet, humanity’s collective intelligence is one of the major reasons we’ve been able to get so far ahead of all other species. And computers will be way better at it than we are. A worldwide network of AI running a particular program could regularly sync with itself so that anything any one computer learned would be instantly uploaded to all other computers. The group could also take on one goal as a unit, because there wouldn’t necessarily be dissenting opinions and motivations and self-interest, like we have within the human population. 10 \n \n AI, which will likely get to AGI by being programmed to self-improve, wouldn’t see “human-level intelligence” as some important milestone—it’s only a relevant marker from our point of view—and wouldn’t have any reason to “stop” at our level. And given the advantages over us that even human intelligence-equivalent AGI would have, it’s pretty obvious that it would only hit human intelligence for a brief instant before racing onwards to the realm of superior-to-human intelligence. \n This may shock the shit out of us when it happens. The reason is that from our perspective, A) while the intelligence of different kinds of animals varies, the main characteristic we’re aware of about any animal’s intelligence is that it’s far lower than ours, and B) we view the smartest humans as WAY smarter than the dumbest humans. Kind of like this: \n \n So as AI zooms upward in intelligence toward us, we’ll see it as simply becoming smarter, for an animal. Then, when it hits the lowest capacity of humanity—Nick Bostrom uses the term “the village idiot”—we’ll be like, “Oh wow, it’s like a dumb human. Cute!” The only thing is, in the grand spectrum of intelligence, all humans, from the village idiot to Einstein, are within a very small range—so just after hitting village idiot level and being declared to be AGI, it’ll suddenly be smarter than Einstein and we won’t know what hit us: \n \n And what happens…after that? \n An Intelligence Explosion \n I hope you enjoyed normal time, because this is when this topic gets unnormal and scary, and it’s gonna stay that way from here forward. I want to pause here to remind you that every single thing I’m going to say is real—real science and real forecasts of the future from a large array of the most respected thinkers and scientists. Just keep remembering that. \n Anyway, as I said above, most of our current models for getting to AGI involve the AI getting there by self-improvement. And once it gets to AGI, even systems that formed and grew through methods that didn’t involve self-improvement would now be smart enough to begin self-improving if they wanted to. 3 \n And here’s where we get to an intense concept: recursive self-improvement. It works like this— \n An AI system at a certain level—let’s say human village idiot—is programmed with the goal of improving its own intelligence. Once it does, it’s smarter— maybe at this point it’s at Einstein’s level—so now when it works to improve its intelligence, with an Einstein-level intellect, it has an easier time and it can make bigger leaps. These leaps make it much smarter than any human, allowing it to make even bigger leaps. As the leaps grow larger and happen more rapidly, the AGI soars upwards in intelligence and soon reaches the superintelligent level of an ASI system. This is called an Intelligence Explosion, 11 and it’s the ultimate example of The Law of Accelerating Returns. \n There is some debate about how soon AI will reach human-level general intelligence. The median year on a survey of hundreds of scientists about when they believed we’d be more likely than not to have reached AGI was 2040 12 —that’s only 25 years from now, which doesn’t sound that huge until you consider that many of the thinkers in this field think it’s likely that the progression from AGI to ASI happens very quickly. Like—this could happen: \n It takes decades for the first AI system to reach low-level general intelligence, but it finally happens. A computer is able to understand the world around it as well as a human four-year-old. Suddenly, within an hour of hitting that milestone, the system pumps out the grand theory of physics that unifies general relativity and quantum mechanics, something no human has been able to definitively do. 90 minutes after that, the AI has become an ASI, 170,000 times more intelligent than a human. \n Superintelligence of that magnitude is not something we can remotely grasp, any more than a bumblebee can wrap its head around Keynesian Economics. In our world, smart means a 130 IQ and stupid means an 85 IQ—we don’t have a word for an IQ of 12,952. \n What we do know is that humans’ utter dominance on this Earth suggests a clear rule: with intelligence comes power. Which means an ASI, when we create it, will be the most powerful being in the history of life on Earth, and all living things, including humans, will be entirely at its whim— and this might happen in the next few decades. \n If our meager brains were able to invent wifi, then something 100 or 1,000 or 1 billion times smarter than we are should have no problem controlling the positioning of each and every atom in the world in any way it likes, at any time—everything we consider magic, every power we imagine a supreme God to have will be as mundane an activity for the ASI as flipping on a light switch is for us. Creating the technology to reverse human aging, curing disease and hunger and even mortality, reprogramming the weather to protect the future of life on Earth—all suddenly possible. Also possible is the immediate end of all life on Earth. As far as we’re concerned, if an ASI comes to being, there is now an omnipotent God on Earth—and the all-important question for us is: \n Will it be a nice God? \n That’s the topic of Part 2 of this post . \n (Sources at the bottom of Part 2 .) \n \n Related Wait But Why Posts \n The Fermi Paradox – Why don’t we see any signs of alien life? \n How (and Why) SpaceX Will Colonize Mars – A post I got to work on with Elon Musk and one that reframed my mental picture of the future. \n Or for something totally different and yet somehow related, Why Procrastinators Procrastinate \n And here’s Year 1 of Wait But Why on an ebook. \n _______ \n If you like Wait But Why, sign up for our email list and we’ll send you new posts when they come out. \n To support Wait But Why, visit our Patreon page .",
        "image": "https://149909199.v2.pressablecdn.com/wp-content/uploads/2015/01/G1.jpg",
        "favicon": "https://149909199.v2.pressablecdn.com/wp-content/themes/waitbutwhy/images/favicon.ico"
      },
      {
        "score": 0.1743553727865219,
        "title": "Why AI Will Save the World",
        "id": "https://a16z.com/ai-will-save-the-world/",
        "url": "https://a16z.com/ai-will-save-the-world/",
        "publishedDate": "2023-06-06T08:45:15.000Z",
        "author": "Marc Andreessen",
        "text": "The era of Artificial Intelligence is here, and boy are people freaking out. \n Fortunately, I am here to bring the good news: AI will not destroy the world, and in fact may save it. \n First, a short description of what AI is : The application of mathematics and software code to teach computers how to understand, synthesize, and generate knowledge in ways similar to how people do it. AI is a computer program like any other – it runs, takes input, processes, and generates output. AI’s output is useful across a wide range of fields, ranging from coding to medicine to law to the creative arts. It is owned by people and controlled by people, like any other technology. \n A shorter description of what AI isn’t : Killer software and robots that will spring to life and decide to murder the human race or otherwise ruin everything, like you see in the movies . \n An even shorter description of what AI could be : A way to make everything we care about better. \n Why AI Can Make Everything We Care About Better \n The most validated core conclusion of social science across many decades and thousands of studies is that human intelligence makes a very broad range of life outcomes better. Smarter people have better outcomes in almost every domain of activity: academic achievement, job performance, occupational status, income, creativity, physical health, longevity, learning new skills, managing complex tasks, leadership, entrepreneurial success, conflict resolution, reading comprehension, financial decision making, understanding others’ perspectives, creative arts, parenting outcomes, and life satisfaction. \n Further, human intelligence is the lever that we have used for millennia to create the world we live in today: science, technology, math, physics, chemistry, medicine, energy, construction, transportation, communication, art, music, culture, philosophy, ethics, morality. Without the application of intelligence on all these domains, we would all still be living in mud huts, scratching out a meager existence of subsistence farming. Instead we have used our intelligence to raise our standard of living on the order of 10,000X over the last 4,000 years. \n What AI offers us is the opportunity to profoundly augment human intelligence to make all of these outcomes of intelligence – and many others, from the creation of new medicines to ways to solve climate change to technologies to reach the stars – much, much better from here. \n AI augmentation of human intelligence has already started – AI is already around us in the form of computer control systems of many kinds, is now rapidly escalating with AI Large Language Models like ChatGPT, and will accelerate very quickly from here – if we let it . \n In our new era of AI: \n \n Every child will have an AI tutor that is infinitely patient, infinitely compassionate, infinitely knowledgeable, infinitely helpful. The AI tutor will be by each child’s side every step of their development, helping them maximize their potential with the machine version of infinite love. \n Every person will have an AI assistant/coach/mentor/trainer/advisor/therapist that is infinitely patient, infinitely compassionate, infinitely knowledgeable, and infinitely helpful. The AI assistant will be present through all of life’s opportunities and challenges, maximizing every person’s outcomes. \n Every scientist will have an AI assistant/collaborator/partner that will greatly expand their scope of scientific research and achievement. Every artist, every engineer, every businessperson, every doctor, every caregiver will have the same in their worlds. \n Every leader of people – CEO, government official, nonprofit president, athletic coach, teacher – will have the same. The magnification effects of better decisions by leaders across the people they lead are enormous, so this intelligence augmentation may be the most important of all. \n Productivity growth throughout the economy will accelerate dramatically, driving economic growth, creation of new industries, creation of new jobs, and wage growth, and resulting in a new era of heightened material prosperity across the planet. \n Scientific breakthroughs and new technologies and medicines will dramatically expand, as AI helps us further decode the laws of nature and harvest them for our benefit. \n The creative arts will enter a golden age, as AI-augmented artists, musicians, writers, and filmmakers gain the ability to realize their visions far faster and at greater scale than ever before. \n I even think AI is going to improve warfare, when it has to happen, by reducing wartime death rates dramatically. Every war is characterized by terrible decisions made under intense pressure and with sharply limited information by very limited human leaders. Now, military commanders and political leaders will have AI advisors that will help them make much better strategic and tactical decisions, minimizing risk, error, and unnecessary bloodshed. \n In short, anything that people do with their natural intelligence today can be done much better with AI, and we will be able to take on new challenges that have been impossible to tackle without AI, from curing all diseases to achieving interstellar travel. \n And this isn’t just about intelligence! Perhaps the most underestimated quality of AI is how humanizing it can be. AI art gives people who otherwise lack technical skills the freedom to create and share their artistic ideas . Talking to an empathetic AI friend really does improve their ability to handle adversity . And AI medical chatbots are already more empathetic than their human counterparts. Rather than making the world harsher and more mechanistic, infinitely patient and sympathetic AI will make the world warmer and nicer. \n \n The stakes here are high. The opportunities are profound. AI is quite possibly the most important – and best – thing our civilization has ever created, certainly on par with electricity and microchips, and probably beyond those. \n The development and proliferation of AI – far from a risk that we should fear – is a moral obligation that we have to ourselves, to our children, and to our future. \n We should be living in a much better world with AI, and now we can. \n TABLE OF CONTENTS So Why The Panic? \n In contrast to this positive view, the public conversation about AI is presently shot through with hysterical fear and paranoia. \n We hear claims that AI will variously kill us all, ruin our society, take all our jobs, cause crippling inequality, and enable bad people to do awful things. \n What explains this divergence in potential outcomes from near utopia to horrifying dystopia? \n Historically, every new technology that matters, from electric lighting to automobiles to radio to the Internet, has sparked a moral panic – a social contagion that convinces people the new technology is going to destroy the world, or society, or both. The fine folks at Pessimists Archive have documented these technology-driven moral panics over the decades; their history makes the pattern vividly clear. It turns out this present panic is not even the first for AI . \n Now, it is certainly the case that many new technologies have led to bad outcomes – often the same technologies that have been otherwise enormously beneficial to our welfare. So it’s not that the mere existence of a moral panic means there is nothing to be concerned about. \n But a moral panic is by its very nature irrational – it takes what may be a legitimate concern and inflates it into a level of hysteria that ironically makes it harder to confront actually serious concerns. \n And wow do we have a full-blown moral panic about AI right now. \n This moral panic is already being used as a motivating force by a variety of actors to demand policy action – new AI restrictions, regulations, and laws. These actors, who are making extremely dramatic public statements about the dangers of AI – feeding on and further inflaming moral panic – all present themselves as selfless champions of the public good. \n But are they? \n And are they right or wrong? \n TABLE OF CONTENTS The Baptists And Bootleggers Of AI \n Economists have observed a longstanding pattern in reform movements of this kind. The actors within movements like these fall into two categories – “Baptists” and “Bootleggers” – drawing on the historical example of the prohibition of alcohol in the United States in the 1920’s : \n Baptists \n“Baptists” are the true believer social reformers who legitimately feel – deeply and emotionally, if not rationally – that new restrictions, regulations, and laws are required to prevent societal disaster. For alcohol prohibition, these actors were often literally devout Christians who felt that alcohol was destroying the moral fabric of society. For AI risk, these actors are true believers that AI presents one or another existential risks – strap them to a polygraph, they really mean it. \n Bootleggers \n“Bootleggers” are the self-interested opportunists who stand to financially profit by the imposition of new restrictions, regulations, and laws that insulate them from competitors. For alcohol prohibition, these were the literal bootleggers who made a fortune selling illicit alcohol to Americans when legitimate alcohol sales were banned. For AI risk, these are CEOs who stand to make more money if regulatory barriers are erected that form a cartel of government-blessed AI vendors protected from new startup and open source competition – the software version of “too big to fail” banks. \n A cynic would suggest that some of the apparent Baptists are also Bootleggers – specifically the ones paid to attack AI by their universities , think tanks , activist groups , and media outlets . If you are paid a salary or receive grants to foster AI panic…you are probably a Bootlegger. \n The problem with the Bootleggers is that they win . The Baptists are naive ideologues, the Bootleggers are cynical operators, and so the result of reform movements like these is often that the Bootleggers get what they want – regulatory capture, insulation from competition, the formation of a cartel – and the Baptists are left wondering where their drive for social improvement went so wrong. \n We just lived through a stunning example of this – banking reform after the 2008 global financial crisis. The Baptists told us that we needed new laws and regulations to break up the “too big to fail” banks to prevent such a crisis from ever happening again. So Congress passed the Dodd-Frank Act of 2010, which was marketed as satisfying the Baptists’ goal, but in reality was coopted by the Bootleggers – the big banks. The result is that the same banks that were “too big to fail” in 2008 are much, much larger now. \n So in practice, even when the Baptists are genuine – and even when the Baptists are right – they are used as cover by manipulative and venal Bootleggers to benefit themselves. \n And this is what is happening in the drive for AI regulation right now. \n However, it isn’t sufficient to simply identify the actors and impugn their motives. We should consider the arguments of both the Baptists and the Bootleggers on their merits. \n TABLE OF CONTENTS AI Risk #1: Will AI Kill Us All? \n The first and original AI doomer risk is that AI will decide to literally kill humanity. \n The fear that technology of our own creation will rise up and destroy us is deeply coded into our culture. The Greeks expressed this fear in the Prometheus Myth – Prometheus brought the destructive power of fire, and more generally technology (“techne”), to man, for which Prometheus was condemned to perpetual torture by the gods. Later, Mary Shelley gave us moderns our own version of this myth in her novel Frankenstein, or, The Modern Prometheus , in which we develop the technology for eternal life, which then rises up and seeks to destroy us. And of course, no AI panic newspaper story is complete without a still image of a gleaming red-eyed killer robot from James Cameron’s Terminator films. \n The presumed evolutionary purpose of this mythology is to motivate us to seriously consider potential risks of new technologies – fire, after all, can indeed be used to burn down entire cities. But just as fire was also the foundation of modern civilization as used to keep us warm and safe in a cold and hostile world, this mythology ignores the far greater upside of most – all? – new technologies, and in practice inflames destructive emotion rather than reasoned analysis. Just because premodern man freaked out like this doesn’t mean we have to; we can apply rationality instead. \n My view is that the idea that AI will decide to literally kill humanity is a profound category error . AI is not a living being that has been primed by billions of years of evolution to participate in the battle for the survival of the fittest, as animals are, and as we are. It is math – code – computers, built by people, owned by people, used by people, controlled by people. The idea that it will at some point develop a mind of its own and decide that it has motivations that lead it to try to kill us is a superstitious handwave. \n In short, AI doesn’t want , it doesn’t have goals , it doesn’t want to kill you , because it’s not alive . And AI is a machine – is not going to come alive any more than your toaster will. \n Now, obviously, there are true believers in killer AI – Baptists – who are gaining a suddenly stratospheric amount of media coverage for their terrifying warnings, some of whom claim to have been studying the topic for decades and say they are now scared out of their minds by what they have learned. Some of these true believers are even actual innovators of the technology. These actors are arguing for a variety of bizarre and extreme restrictions on AI ranging from a ban on AI development , all the way up to military airstrikes on datacenters and nuclear war . They argue that because people like me cannot rule out future catastrophic consequences of AI, that we must assume a precautionary stance that may require large amounts of physical violence and death in order to prevent potential existential risk. \n My response is that their position is non-scientific – What is the testable hypothesis? What would falsify the hypothesis? How do we know when we are getting into a danger zone? These questions go mainly unanswered apart from “You can’t prove it won’t happen!” In fact, these Baptists’ position is so non-scientific and so extreme – a conspiracy theory about math and code – and is already calling for physical violence, that I will do something I would normally not do and question their motives as well. \n Specifically, I think three things are going on: \n First, recall that John Von Neumann responded to Robert Oppenheimer’s famous hand-wringing about his role creating nuclear weapons – which helped end World War II and prevent World War III – with, “Some people confess guilt to claim credit for the sin.” What is the most dramatic way one can claim credit for the importance of one’s work without sounding overtly boastful? This explains the mismatch between the words and actions of the Baptists who are actually building and funding AI – watch their actions, not their words. (Truman was harsher after meeting with Oppenheimer: “Don’t let that crybaby in here again.” ) \n Second, some of the Baptists are actually Bootleggers. There is a whole profession of “AI safety expert”, “AI ethicist”, “AI risk researcher”. They are paid to be doomers, and their statements should be processed appropriately. \n Third, California is justifiably famous for our many thousands of cults , from EST to the Peoples Temple, from Heaven’s Gate to the Manson Family. Many, although not all, of these cults are harmless, and maybe even serve a purpose for alienated people who find homes in them. But some are very dangerous indeed, and cults have a notoriously hard time straddling the line that ultimately leads to violence and death . \n And the reality, which is obvious to everyone in the Bay Area but probably not outside of it, is that “AI risk” has developed into a cult , which has suddenly emerged into the daylight of global press attention and the public conversation. This cult has pulled in not just fringe characters, but also some actual industry experts and a not small number of wealthy donors – including, until recently, Sam Bankman-Fried . And it’s developed a full panoply of cult behaviors and beliefs. \n This cult is why there are a set of AI risk doomers who sound so extreme – it’s not that they actually have secret knowledge that make their extremism logical, it’s that they’ve whipped themselves into a frenzy and really are…extremely extreme. \n It turns out that this type of cult isn’t new – there is a longstanding Western tradition of millenarianism , which generates apocalypse cults. The AI risk cult has all the hallmarks of a millenarian apocalypse cult. From Wikipedia, with additions by me: \n “Millenarianism is the belief by a group or movement [AI risk doomers] in a coming fundamental transformation of society [the arrival of AI], after which all things will be changed [AI utopia, dystopia, and/or end of the world]. Only dramatic events [AI bans, airstrikes on datacenters, nuclear strikes on unregulated AI] are seen as able to change the world [prevent AI] and the change is anticipated to be brought about, or survived, by a group of the devout and dedicated. In most millenarian scenarios, the disaster or battle to come [AI apocalypse, or its prevention] will be followed by a new, purified world [AI bans] in which the believers will be rewarded [or at least acknowledged to have been correct all along].” \n This apocalypse cult pattern is so obvious that I am surprised more people don’t see it. \n Don’t get me wrong, cults are fun to hear about, their written material is often creative and fascinating , and their members are engaging at dinner parties and on TV . But their extreme beliefs should not determine the future of laws and society – obviously not. \n TABLE OF CONTENTS AI Risk #2: Will AI Ruin Our Society? \n The second widely mooted AI risk is that AI will ruin our society, by generating outputs that will be so “harmful”, to use the nomenclature of this kind of doomer, as to cause profound damage to humanity, even if we’re not literally killed. \n Short version: If the murder robots don’t get us, the hate speech and misinformation will. \n This is a relatively recent doomer concern that branched off from and somewhat took over the “AI risk” movement that I described above. In fact, the terminology of AI risk recently changed from “AI safety” – the term used by people who are worried that AI would literally kill us – to “AI alignment” – the term used by people who are worried about societal “harms”. The original AI safety people are frustrated by this shift, although they don’t know how to put it back in the box – they now advocate that the actual AI risk topic be renamed “AI notkilleveryoneism”, which has not yet been widely adopted but is at least clear. \n The tipoff to the nature of the AI societal risk claim is its own term, “AI alignment”. Alignment with what? Human values. Whose human values? Ah, that’s where things get tricky. \n As it happens, I have had a front row seat to an analogous situation – the social media “trust and safety” wars. As is now obvious , social media services have been under massive pressure from governments and activists to ban, restrict, censor, and otherwise suppress a wide range of content for many years. And the same concerns of “hate speech” (and its mathematical counterpart, “algorithmic bias”) and “misinformation” are being directly transferred from the social media context to the new frontier of “AI alignment”. \n My big learnings from the social media wars are: \n On the one hand, there is no absolutist free speech position. First, every country, including the United States, makes at least some content illegal . Second, there are certain kinds of content, like child pornography and incitements to real world violence, that are nearly universally agreed to be off limits – legal or not – by virtually every society. So any technological platform that facilitates or generates content – speech – is going to have some restrictions. \n On the other hand, the slippery slope is not a fallacy, it’s an inevitability. Once a framework for restricting even egregiously terrible content is in place – for example, for hate speech, a specific hurtful word, or for misinformation, obviously false claims like “ the Pope is dead ” – a shockingly broad range of government agencies and activist pressure groups and nongovernmental entities will kick into gear and demand ever greater levels of censorship and suppression of whatever speech they view as threatening to society and/or their own personal preferences. They will do this up to and including in ways that are nakedly felony crimes . This cycle in practice can run apparently forever, with the enthusiastic support of authoritarian hall monitors installed throughout our elite power structures. This has been cascading for a decade in social media and with only certain exceptions continues to get more fervent all the time. \n And so this is the dynamic that has formed around “AI alignment” now. Its proponents claim the wisdom to engineer AI-generated speech and thought that are good for society, and to ban AI-generated speech and thoughts that are bad for society. Its opponents claim that the thought police are breathtakingly arrogant and presumptuous – and often outright criminal, at least in the US – and in fact are seeking to become a new kind of fused government-corporate-academic authoritarian speech dictatorship ripped straight from the pages of George Orwell’s 1984 . \n As the proponents of both “trust and safety” and “AI alignment” are clustered into the very narrow slice of the global population that characterizes the American coastal elites – which includes many of the people who work in and write about the tech industry – many of my readers will find yourselves primed to argue that dramatic restrictions on AI output are required to avoid destroying society. I will not attempt to talk you out of this now, I will simply state that this is the nature of the demand, and that most people in the world neither agree with your ideology nor want to see you win . \n If you don’t agree with the prevailing niche morality that is being imposed on both social media and AI via ever-intensifying speech codes, you should also realize that the fight over what AI is allowed to say/generate will be even more important – by a lot – than the fight over social media censorship. AI is highly likely to be the control layer for everything in the world. How it is allowed to operate is going to matter perhaps more than anything else has ever mattered. You should be aware of how a small and isolated coterie of partisan social engineers are trying to determine that right now, under cover of the age-old claim that they are protecting you. \n In short, don’t let the thought police suppress AI. \n TABLE OF CONTENTS AI Risk #3: Will AI Take All Our Jobs? \n The fear of job loss due variously to mechanization, automation, computerization, or AI has been a recurring panic for hundreds of years, since the original onset of machinery such as the mechanical loom . Even though every new major technology has led to more jobs at higher wages throughout history, each wave of this panic is accompanied by claims that “this time is different” – this is the time it will finally happen, this is the technology that will finally deliver the hammer blow to human labor. And yet, it never happens. \n We’ve been through two such technology-driven unemployment panic cycles in our recent past – the outsourcing panic of the 2000’s, and the automation panic of the 2010’s. Notwithstanding many talking heads, pundits, and even tech industry executives pounding the table throughout both decades that mass unemployment was near, by late 2019 – right before the onset of COVID – the world had more jobs at higher wages than ever in history. \n Nevertheless this mistaken idea will not die . \n And sure enough, it’s back . \n This time , we finally have the technology that’s going to take all the jobs and render human workers superfluous – real AI. Surely this time history won’t repeat, and AI will cause mass unemployment – and not rapid economic, job, and wage growth – right? \n No, that’s not going to happen – and in fact AI, if allowed to develop and proliferate throughout the economy, may cause the most dramatic and sustained economic boom of all time, with correspondingly record job and wage growth – the exact opposite of the fear. And here’s why. \n The core mistake the automation-kills-jobs doomers keep making is called the Lump Of Labor Fallacy . This fallacy is the incorrect notion that there is a fixed amount of labor to be done in the economy at any given time, and either machines do it or people do it – and if machines do it, there will be no work for people to do. \n The Lump Of Labor Fallacy flows naturally from naive intuition, but naive intuition here is wrong. When technology is applied to production, we get productivity growth – an increase in output generated by a reduction in inputs. The result is lower prices for goods and services. As prices for goods and services fall, we pay less for them, meaning that we now have extra spending power with which to buy other things . This increases demand in the economy, which drives the creation of new production – including new products and new industries – which then creates new jobs for the people who were replaced by machines in prior jobs. The result is a larger economy with higher material prosperity, more industries, more products, and more jobs. \n But the good news doesn’t stop there. We also get higher wages. This is because, at the level of the individual worker, the marketplace sets compensation as a function of the marginal productivity of the worker . A worker in a technology-infused business will be more productive than a worker in a traditional business. The employer will either pay that worker more money as he is now more productive, or another employer will, purely out of self interest. The result is that technology introduced into an industry generally not only increases the number of jobs in the industry but also raises wages. \n To summarize, technology empowers people to be more productive. This causes the prices for existing goods and services to fall, and for wages to rise. This in turn causes economic growth and job growth, while motivating the creation of new jobs and new industries. If a market economy is allowed to function normally and if technology is allowed to be introduced freely, this is a perpetual upward cycle that never ends. For, as Milton Friedman observed, “Human wants and needs are endless” – we always want more than we have. A technology-infused market economy is the way we get closer to delivering everything everyone could conceivably want, but never all the way there. And that is why technology doesn’t destroy jobs and never will. \n These are such mindblowing ideas for people who have not been exposed to them that it may take you some time to wrap your head around them. But I swear I’m not making them up – in fact you can read all about them in standard economics textbooks. I recommend the chapter The Curse of Machinery in Henry Hazlitt’s Economics In One Lesson , and Frederic Bastiat’s satirical Candlemaker’s Petition to blot out the sun due to its unfair competition with the lighting industry, here modernized for our times . \n But this time is different , you’re thinking. This time, with AI, we have the technology that can replace ALL human labor. \n But, using the principles I described above, think of what it would mean for literally all existing human labor to be replaced by machines. \n It would mean a takeoff rate of economic productivity growth that would be absolutely stratospheric, far beyond any historical precedent. Prices of existing goods and services would drop across the board to virtually zero. Consumer welfare would skyrocket. Consumer spending power would skyrocket. New demand in the economy would explode. Entrepreneurs would create dizzying arrays of new industries, products, and services, and employ as many people and AI as they could as fast as possible to meet all the new demand. \n Suppose AI once again replaces that labor? The cycle would repeat, driving consumer welfare, economic growth, and job and wage growth even higher. It would be a straight spiral up to a material utopia that neither Adam Smith or Karl Marx ever dared dream of. \n We should be so lucky. \n TABLE OF CONTENTS AI Risk #4 Will AI Lead To Crippling Inequality? \n Speaking of Karl Marx, the concern about AI taking jobs segues directly into the next claimed AI risk, which is, OK, Marc, suppose AI does take all the jobs, either for bad or for good. Won’t that result in massive and crippling wealth inequality, as the owners of AI reap all the economic rewards and regular people get nothing? \n As it happens, this was a central claim of Marxism, that the owners of the means of production – the bourgeoisie – would inevitably steal all societal wealth from the people who do the actual work – the proletariat. This is another fallacy that simply will not die no matter how often it’s disproved by reality. But let’s drive a stake through its heart anyway. \n The flaw in this theory is that, as the owner of a piece of technology, it’s not in your own interest to keep it to yourself – in fact the opposite, it’s in your own interest to sell it to as many customers as possible. The largest market in the world for any product is the entire world, all 8 billion of us. And so in reality, every new technology – even ones that start by selling to the rarefied air of high-paying big companies or wealthy consumers – rapidly proliferates until it’s in the hands of the largest possible mass market, ultimately everyone on the planet. \n The classic example of this was Elon Musk’s so-called “secret plan” – which he naturally published openly – for Tesla in 2006: \n Step 1, Build [expensive] sports car \n Step 2, Use that money to build an affordable car \n Step 3, Use that money to build an even more affordable car \n …which is of course exactly what he’s done, becoming the richest man in the world as a result. \n That last point is key. Would Elon be even richer if he only sold cars to rich people today? No. Would he be even richer than that if he only made cars for himself? Of course not. No, he maximizes his own profit by selling to the largest possible market, the world. \n In short, everyone gets the thing – as we saw in the past with not just cars but also electricity, radio, computers, the Internet, mobile phones, and search engines. The makers of such technologies are highly motivated to drive down their prices until everyone on the planet can afford them. This is precisely what is already happening in AI – it’s why you can use state of the art generative AI not just at low cost but even for free today in the form of Microsoft Bing and Google Bard – and it is what will continue to happen. Not because such vendors are foolish or generous but precisely because they are greedy – they want to maximize the size of their market, which maximizes their profits. \n So what happens is the opposite of technology driving centralization of wealth – individual customers of the technology, ultimately including everyone on the planet, are empowered instead, and capture most of the generated value . As with prior technologies, the companies that build AI – assuming they have to function in a free market – will compete furiously to make this happen. \n Marx was wrong then, and he’s wrong now. \n This is not to say that inequality is not an issue in our society. It is, it’s just not being driven by technology, it’s being driven by the reverse , by the sectors of the economy that are the most resistant to new technology, that have the most government intervention to prevent the adoption of new technology like AI – specifically housing, education, and health care. The actual risk of AI and inequality is not that AI will cause more inequality but rather that we will not allow AI to be used to reduce inequality . \n AI Risk #5: Will AI Lead to Bad People Doing Bad Things? \n So far I have explained why four of the five most often proposed risks of AI are not actually real – AI will not come to life and kill us, AI will not ruin our society, AI will not cause mass unemployment, and AI will not cause an ruinous increase in inequality. But now let’s address the fifth, the one I actually agree with: AI will make it easier for bad people to do bad things. \n In some sense this is a tautology. Technology is a tool. Tools, starting with fire and rocks, can be used to do good things – cook food and build houses – and bad things – burn people and bludgeon people. Any technology can be used for good or bad. Fair enough. And AI will make it easier for criminals, terrorists, and hostile governments to do bad things, no question. \n This causes some people to propose, well, in that case, let’s not take the risk, let’s ban AI now before this can happen . Unfortunately, AI is not some esoteric physical material that is hard to come by, like plutonium. It’s the opposite, it’s the easiest material in the world to come by – math and code. \n The AI cat is obviously already out of the bag. You can learn how to build AI from thousands of free online courses, books, papers, and videos, and there are outstanding open source implementations proliferating by the day . AI is like air – it will be everywhere. The level of totalitarian oppression that would be required to arrest that would be so draconian – a world government monitoring and controlling all computers? jackbooted thugs in black helicopters seizing rogue GPUs? – that we would not have a society left to protect. \n So instead, there are two very straightforward ways to address the risk of bad people doing bad things with AI, and these are precisely what we should focus on. \n First, we have laws on the books to criminalize most of the bad things that anyone is going to do with AI. Hack into the Pentagon? That’s a crime. Steal money from a bank? That’s a crime. Create a bioweapon? That’s a crime. Commit a terrorist act? That’s a crime. We can simply focus on preventing those crimes when we can, and prosecuting them when we cannot. We don’t even need new laws – I’m not aware of a single actual bad use for AI that’s been proposed that’s not already illegal. And if a new bad use is identified, we ban that use. QED. \n But you’ll notice what I slipped in there – I said we should focus first on preventing AI-assisted crimes before they happen – wouldn’t such prevention mean banning AI? Well, there’s another way to prevent such actions, and that’s by using AI as a defensive tool . The same capabilities that make AI dangerous in the hands of bad guys with bad goals make it powerful in the hands of good guys with good goals – specifically the good guys whose job it is to prevent bad things from happening. \n For example, if you are worried about AI generating fake people and fake videos, the answer is to build new systems where people can verify themselves and real content via cryptographic signatures. Digital creation and alteration of both real and fake content was already here before AI; the answer is not to ban word processors and Photoshop – or AI – but to use technology to build a system that actually solves the problem. \n And so, second, let’s mount major efforts to use AI for good, legitimate, defensive purposes. Let’s put AI to work in cyberdefense, in biological defense, in hunting terrorists, and in everything else that we do to keep ourselves, our communities, and our nation safe. \n There are already many smart people in and out of government doing exactly this, of course – but if we apply all of the effort and brainpower that’s currently fixated on the futile prospect of banning AI to using AI to protect against bad people doing bad things, I think there’s no question a world infused with AI will be much safer than the world we live in today. \n TABLE OF CONTENTS The Actual Risk Of Not Pursuing AI With Maximum Force And Speed \n There is one final, and real, AI risk that is probably the scariest at all: \n AI isn’t just being developed in the relatively free societies of the West, it is also being developed by the Communist Party of the People’s Republic of China. \n China has a vastly different vision for AI than we do – they view it as a mechanism for authoritarian population control, full stop. They are not even being secretive about this, they are very clear about it , and they are already pursuing their agenda. And they do not intend to limit their AI strategy to China – they intend to proliferate it all across the world , everywhere they are powering 5G networks, everywhere they are loaning Belt And Road money, everywhere they are providing friendly consumer apps like Tiktok that serve as front ends to their centralized command and control AI. \n The single greatest risk of AI is that China wins global AI dominance and we – the United States and the West – do not. \n I propose a simple strategy for what to do about this – in fact, the same strategy President Ronald Reagan used to win the first Cold War with the Soviet Union. \n “We win, they lose.” \n Rather than allowing ungrounded panics around killer AI, “harmful” AI, job-destroying AI, and inequality-generating AI to put us on our back feet, we in the United States and the West should lean into AI as hard as we possibly can. \n We should seek to win the race to global AI technological superiority and ensure that China does not. \n In the process, we should drive AI into our economy and society as fast and hard as we possibly can, in order to maximize its gains for economic productivity and human potential. \n This is the best way both to offset the real AI risks and to ensure that our way of life is not displaced by the much darker Chinese vision . \n TABLE OF CONTENTS What Is To Be Done? \n I propose a simple plan: \n \n Big AI companies should be allowed to build AI as fast and aggressively as they can – but not allowed to achieve regulatory capture, not allowed to establish a government-protect cartel that is insulated from market competition due to incorrect claims of AI risk. This will maximize the technological and societal payoff from the amazing capabilities of these companies, which are jewels of modern capitalism. \n Startup AI companies should be allowed to build AI as fast and aggressively as they can. They should neither confront government-granted protection of big companies, nor should they receive government assistance. They should simply be allowed to compete. If and as startups don’t succeed, their presence in the market will also continuously motivate big companies to be their best – our economies and societies win either way. \n Open source AI should be allowed to freely proliferate and compete with both big AI companies and startups. There should be no regulatory barriers to open source whatsoever. Even when open source does not beat companies, its widespread availability is a boon to students all over the world who want to learn how to build and use AI to become part of the technological future, and will ensure that AI is available to everyone who can benefit from it no matter who they are or how much money they have. \n To offset the risk of bad people doing bad things with AI, governments working in partnership with the private sector should vigorously engage in each area of potential risk to use AI to maximize society’s defensive capabilities. This shouldn’t be limited to AI-enabled risks but also more general problems such as malnutrition, disease, and climate. AI can be an incredibly powerful tool for solving problems, and we should embrace it as such. \n To prevent the risk of China achieving global AI dominance, we should use the full power of our private sector, our scientific establishment, and our governments in concert to drive American and Western AI to absolute global dominance, including ultimately inside China itself. We win, they lose. \n \n And that is how we use AI to save the world. \n It’s time to build. \n Legends and Heroes \n I close with two simple statements. \n The development of AI started in the 1940’s, simultaneous with the invention of the computer . The first scientific paper on neural networks – the architecture of the AI we have today – was published in 1943 . Entire generations of AI scientists over the last 80 years were born, went to school, worked, and in many cases passed away without seeing the payoff that we are receiving now. They are legends, every one. \n Today, growing legions of engineers – many of whom are young and may have had grandparents or even great-grandparents involved in the creation of the ideas behind AI – are working to make AI a reality, against a wall of fear-mongering and doomerism that is attempting to paint them as reckless villains. I do not believe they are reckless or villains. They are heroes, every one. My firm and I are thrilled to back as many of them as we can, and we will stand alongside them and their work 100%.",
        "image": "https://d1lamhf6l6yk6d.cloudfront.net/uploads/2023/06/AI-Saves-The-World_Yoast-FB.jpg",
        "favicon": "https://d1lamhf6l6yk6d.cloudfront.net/uploads/2024/03/favicon.png"
      },
      {
        "score": 0.17420703172683716,
        "title": "I need AI",
        "id": "https://coryd.dev/posts/2024/i-need-ai/",
        "url": "https://coryd.dev/posts/2024/i-need-ai/",
        "publishedDate": "2024-02-25T00:00:00.000Z",
        "author": "Cory Dransfeldt",
        "text": "I need AI to waste energy. I need it to deprive vulnerable communities of water so that it can be used to cool new data centers. I need AI to make up answers to my questions. I need AI to waste energy. I need it to deprive vulnerable communities of water so that it can be used to cool new data centers. I need AI to make up answers to my questions. I need AI to make up APIs and lower my code quality. I need it to repeatedly give me wrong answers until I realize I could have just written it myself. I need it to fail to answer customer questions and make up company policies. I need AI to ignore qualified job candidates. I need AI to attend meetings that could have been an email and write emails that could have been a Slack message. I need AI to produce generic art for unimaginative creators. I need AI to exploit publicly available content on the web without compensating any creators. I need AI to deskill knowledge work and weaken labor. I need AI to miscategorize images so we can apologize for it after the damage has been done. I need AI to pilot autonomous vehicles that injure pedestrians and take jobs from taxi drivers and truckers. I need AI to create more invasive and insidious advertising for things I don't need. I need AI to do even more damage to the journalism industry and create content nobody can trust. I need AI to produce gimmicky videos that threaten the jobs of creators and secure funding for the already wealthy. I need AI that can't taste to create recipes. I need AI that's never traveled to plan my vacation. I need AI that can't appreciate music to recommend music for me. I need AI to replace web search with answers I can't trust and with no benefit to content creators. Life is short and I need AI to flood what time I have with more garbage to sift through. Who asked for this?",
        "image": "https://cdn.coryd.dev/assets/avatar.png",
        "favicon": "https://cdn.coryd.dev/assets/icons/favicon.ico?v=19.10.10"
      },
      {
        "score": 0.1716436892747879,
        "title": "What Artificial Intelligence Is Not - BLARB",
        "id": "https://blog.lareviewofbooks.org/provocations/artificial-intelligence/",
        "url": "https://blog.lareviewofbooks.org/provocations/artificial-intelligence/",
        "publishedDate": "2020-02-22T18:30:30.000Z",
        "author": "",
        "text": "Artificial intelligence is not one thing. \n Artificial intelligence is not an algorithm . An algorithm is a set method for completing a task. Typically, we talk about algorithms that are implemented by a computer and written in computer code. But algorithms can also be written in math, like the quadratic formula or the equation to calculate area of a circle; or they can be written in natural language, like a chocolate chip cookie recipe or instructions for assembling a desk. \n Artificial intelligence is not a model . A model is a set of mathematical formulas that is trained on data to create predictions and derive conclusions by looking at the data and finding patterns. To train a model, a task is assigned on a massive amount of data. This might be picking out all the kitten photos out of 5 million animal pictures. Or it could be identifying whether people are looking for the state, the US capital, or the historic figure when they type in “Washington.” The model then builds its own process as it attempts to complete the task and work through the data. This is called machine learning. Machine learning is often called a black box. The term black box is accurate but incomplete. Imagine you put a child in the middle of the woods a mile from her house, and tell her to find her way home. You attach a GPS beacon to her so you can track her progress. Now imagine you did the exact same experiment but with a dog. The dog takes a very different path home than the girl, but still completes the task. Through the tracking data you can see perfectly the two routes taken: one by the girl and one by the dog. You know how they traveled home. But, unlike the girl, the dog cannot tell you why it made the choices it made in selecting its route. Like the dog, machine learning is unable to provide reasons and explanation for how it does things. \n Artificial intelligence is not robots . It can be robots, but it is not just robots. Robots are machines built and designed by humans that do automatic tasks. If something is automatic it means that it performs a programmed process. That programmed process implemented in a robot is written and designed by humans. Some robots today contain some aspects of artificial intelligence, but most robots are still just repeating the same immutable actions. \n Artificial intelligence is not neutral . Artificial intelligence is built by humans and those humans write algorithms. Those humans are often men and they are often straight and they often come from privileged backgrounds. Often, the data that these humans use to train the algorithms they write to build artificial intelligence is based on a population that is often randomly gathered from what is available, which might end up looking a lot like the humans who wrote the algorithms. This leads to various problems with artificial intelligence when it confronts humans that don’t look, behave, or present in the same way as the humans who wrote or were used to train the artificial intelligence. This is not necessarily the fault of those humans, and this does not mean that they should be excluded from work on artificial intelligence. Humans, like machines, are flawed. But humans, like artificial intelligence, can try to learn from mistakes. Humans can build better artificial intelligence if they have humans build it that look like all different kinds of humans and train it on human data that represents all different kinds of humans. \n Artificial intelligence is not the singularity . The singularity is a term that refers to a hypothesized moment in the future in which technological growth and machine intelligence can no longer be controlled by humans. Science fiction authors have written about the singularity for a long time. Philosophers, ethicists, technologists, and people with blogs have devoted a lot of energy and time to fearing or not-fearing the singularity. The singularity might never happen. Or it might. But if you are in a sinking ship and taking on water, it might be better to spend your time on pumping, fixing holes, and finding lifeboats than worrying about a pirate attack. So too is it perhaps more prudent to spend time on the urgent and knowable problems of AI than those imagine ones that might not ever come to be. \n Artificial intelligence is not, maybe, a distinction that will matter . Artificial intelligence is a term in contrast with natural intelligence, which is how we think of the intelligence displayed by humans and sometimes certain animals. But if a poem is written by a computer using artificial intelligence and the poem is so beautiful it makes you cry, does it make the poem mean less if you know the source is “artificial”? If a robot uses artificial intelligence to listen to and talk to an elderly woman, does it make the affection that woman feels for the robot mean less? Do things mean more again if you remember that the poem computer and the robot friend were running algorithms written by humans based on data sets constructed from humans? And if it’s really all about what humans are building and how humans react to what is built, isn’t the true question not about what artificial intelligence is or is not, but what it could ever be? \n Kate Klonick is an assistant professor at St. John’s Law School, where she teaches property, internet law, and a seminar on information privacy. Her work has appeared in the Harvard Law Review, the Georgetown Law Journal , the Maryland Law Review , and is forthcoming in the Southern California Law Review and the Yale Law Journal . She is on Twitter at @klonick .",
        "image": "https://blog.lareviewofbooks.org/wp-content/uploads/2020/02/klonick_cover.jpg",
        "favicon": "https://blog.lareviewofbooks.org/wp-content/uploads/2017/12/cropped-blarb-icon-1-32x32.png"
      },
      {
        "score": 0.1714244782924652,
        "title": "Information Theory: Obstacles To True Artificial Intelligence",
        "id": "https://blog.paulbohm.com/p/information-theory-obstacles-to-true-artificial-intelligence-65727c1c38c",
        "url": "https://blog.paulbohm.com/p/information-theory-obstacles-to-true-artificial-intelligence-65727c1c38c",
        "publishedDate": "2016-11-20T09:45:47.000Z",
        "author": "Paul Bohm",
        "text": "Information Theory: Obstacles To True Artificial Intelligence With the Singularity Summit coming to San Francisco tomorrow, a lot of people are wondering about how close we are to successfully implementing Artificial Intelligence. Probably you’ve seen a graph like the above before. Computers are getting faster. Does this mean we’re getting closer to true Artificial Intelligence? Not necessarily. As I’ll show in the following paragraphs, both Artificial Intelligence and the less ambitious Machine Learning (ML) aren’t computationally but data bounded. This has profound implications for viable avenues for research and development. Generally speaking, the goal of AI and ML research is to acquire a digitally executable copy of a model behavior. The specific model behavior desired can range from classifying (“are there humans in the picture?”, “ixs this news article about economics?”) to predicting values (“how much does a house in this neighborhood sell for?”), to making conversation like a human would (“a chatbot”). Any such observable behavior can be mathematically described as a mapping from a set of finite input sequences to corresponding outputs, or in other words: a function. And any such function can be translated into a corresponding computer program. That is, we can represent the model we want to learn (“respond to lines of chat messages like a human would” in the chat bot case) as a function, and then in turn said function as a computer program. The length of the shortest computer program that provides a mapping equivalent to the model function, can be seen as a measure of the complexity of the model. (For example the shortest program that performs addition on input numbers obviously is shorter — that is, has lower complexity — than the shortest program that parses human acoustic speech) Now, the goal of machine learning techniques often is seen as helping us acquire a computer program equivalent to the model we want to learn. In general, this feat is performed like this: A human writes a machine learning program L, which given training data T, generates desired program M. The sleight of hand, in case you haven’t noticed, is that instead of writing M directly, the human has simply decided to write L which writes M for him. The problem is that if the shortest L is shorter than the shortest M, we’re inevitably missing information — there’s many many different candidate M we could be writing — but which one specifically are we supposed to write? We need to acquire this information from somewhere, and the only place other than L we can get this information from is our training data T. T is a list of examples of M in action. This could be a data set of (houses, their location, and their sales price), or a list of pictures and whether they contain a human face, or log files of human chatroom conversation. The problem is that learning from examples without prior knowledge is notoriously difficult — to learn complex concepts we need an impractical amount of examples. (as proven in [1]) So let us recap here for a second: All the information that goes into producing M is a generator/machine learning function L (written by a human), and training data T (acquired through interaction with humans). The information encoded in L combined with the information extracted from T needs to at least match or exceed the information encoded in M. The only reasonable way to extract information from T is to encode information about its underlying pattern in L. That is: All the work needs to be done by humans. We face the following conundrum: To get a better model, we either need make the learning algorithm more complex/write more code (which is human work), or we need to gather more or better sample data (which requires human work as well). The complexity of the model M that we want to learn can’t be essentially reduced — it needs to be captured in a combination of program and training data. Since these are information theoretical limitations, Moore’s law and faster computers do not help. Even a computationally unbounded supercomputer would not be able to break the laws of information theory. Attempts to reduce the necessary work essentially by sophisticated machine learning methods can be compared with attempting to build a perpetuum mobile. So how do we improve computer models? More often than not the answer is through more and better data. Humans often have knowledge about underlying relationships between data, which helps them write better M and L. But for problems where they themselves haven’t understood the underlying relationships, they are no help. Since the contribution of machine learning techniques to knowledge acquisition tends asymptotically to zero from a viewpoint of algorithmic information theory , more or better data is needed. Interestingly enough economists Mises and Hayek have discovered the necessity of data to decision making already in the 1920s in their treatises on the Economic Calculation Problem. Since markets force humans to interact, they reveal preferences through interaction. When removing the market, this preference data is never revealed. Thus the market is required to acquire the extract the data needed to do economic planning from people’s heads. In a similar fashion the problem of AI needs to be approached with additional data. Either this data is directly extracted from a physical medium we know it to be encoded in (e.g. by scanning our body and brain), or we need to find ways to scale the amount of relevant data available to machine learning systems significantly. A second way besides scanning would be simulation: If we’re fundamentally data limited, and we can’t acquire sufficient data from the physical world through adding sensors or collecting data, maybe we can make computers collect data from world’s of their own creation. In the simple case this would amount to computers playing games of parametrizable difficulty similar to Chess or Go against each other. It’s far easier to specify a game like Go than it is to fully understand it — so in theory we can let the computers explore a search space that provides its own data. The problem is that exploration of such a kind might over time approximate the complexity of a full particle simulation. It’s not clear how deep we’d have to search to find the kind of consciousness patterns we’re looking for. But we need to keep in mind that we might need to simulate an entire universe to find the patterns we are looking for. Embodiment is the remaining third way, and one that had been chosen for humanity long before it made its own decisions. Here new bodies come loaded with both sensors and computational power. Since structurally there’s no clear boundary between the processes of cognition and living, this could be considered part of an efficient search process for consciousness throughout the entire universe. In this argument, our physical embodiment is actually a fairly efficient vessel, and humanity is more likely than not to become a first super-intelligence. The internet then amounts to said super-intelligence performing brain-surgery on itself, because changing the plumbing makes it capable of acting in a smarter more coordinated fashion, and those of us who develop better information routing technology simply continue the procedure. In Summary: The real problem of creating good AI is data and complexity, not computational power. The information theoretical bounds described above apply equally to computers of finite and infinite computational power. In essence, an equivalent question to “can we learn intelligence by observing intelligent behavior” is: Can we learn about the structure of living organisms simply by looking at recorded DNA data. If so, how many DNA samples do we need? A lot. If we could examine the physical organism directly instead, we’d learn a lot faster. It’ll be interesting to see where all this goes. Post-Script: I’ve omitted formality in my arguments for the sake of reaching an audience. For a formal argument please see A. Hoffmann, ECAI 1990 and A. Hoffmann, Minds &amp; Machines 2010 , whom I’ve often simply paraphrased and in two cases quoted verbatim. [1] A Ehrenfeucht, D Haussler, M Kearns… 1989 — A general lower bound on the number of examples needed for learnin",
        "image": "https://substackcdn.com/image/fetch/w_1200,h_600,c_fill,f_jpg,q_auto:good,fl_progressive:steep,g_auto/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc47c6764-3e25-4348-8ebf-5e33fe0f1e85_469x400.png",
        "favicon": "https://substackcdn.com/image/fetch/f_auto,q_auto:good,fl_progressive:steep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F01aad1a3-e216-422a-a492-a60d74dcd5e8%2Ffavicon-32x32.png"
      },
      {
        "score": 0.1713346242904663,
        "title": "Why AI is Exploding NOW!",
        "id": "https://www.diamandis.com/blog/ai-exploding-now",
        "url": "https://www.diamandis.com/blog/ai-exploding-now",
        "publishedDate": "2023-04-23T00:00:00.000Z",
        "author": "Peter H. Diamandis",
        "text": "AI was first discussed in 1956 at a Dartmouth Conference 70 years old. And the first deep-learning-like algorithms were described in 1965. \n So Why is AI Exploding Now? \n There are four reasons that are important to understand: \n #1. Computational Power : While the power of computation has been doubling every 12 to 24 months for the past 50 years (Moore’s Law), it is just recently (the last 5 years) that there is enough computational power to run today’s deep learning algorithms. \n #2. Massive Amount of Labeled Data : Global data has been doubling every two years, and is expected to reach 175 Zettabytes (175 billion-million-megabytes) in 2025. Not only is it a large amount of data, but it is also labeled data, enabling the training of today’s large language models (LLMs). \n #3. Demonetized Training Costs : The cost to train these AI systems has plummeted 99.5% in the last five years. That’s extraordinary . \n #4. Massive Investments : In 2021, corporations invested $160 billion in AI. By 2030, the global AI market is projected to be worth $1.6 trillion . \n We saw ChatGPT go from 0 to 100 million users in just two months . \n Proof of AI Domination: \n Need more evidence of the power of AI? Below are just a few recent developments, what I call “Proof of AI Domination”: \n \n ChatGPT passes the Bar Exam, US Medical License Exam, and Wharton’s MBA Final Exam. For comparison, the US Medical License Exam normally requires four years of medical school and then a few more years of residency to pass. ChatGPT did it in two months . \n GPT-4, OpenAI’s newest model released last month, then beat us humans by 75% or more on a range of other exams: from the LSAT and the GRE, to the SAT and the AP Math exam. \n An AI system defeats top doctors in tumor diagnosis competition in China. The AI correctly diagnosed 85% in just 18 minutes, compared to top neurologists who achieved 64% accuracy in 50 minutes. Soon, our best diagnosticians are going to be AIs. \n DeepMind’s AlphaCode AI outperforms human coders . When pitted against over 5,000 human participants, the AI outperformed 45% of expert programmers. \n An AI can clone your voice from just 3 seconds of audio. Microsoft’s AI Vall-E can synthesize your voice using only a 3 second audio sample and preserve intonation, emotion, and voice style. \n The recently released semi-autonomous AI Auto-GPT removes the need for writing prompts altogether . Auto-GPT, which is based on GPT-4, works through goal-setting, with the AI generating and completing tasks once a given goal has been specified—from creating and completing to-do lists, to ordering food and booking flights. \n AI designs a potential cancer drug in 30 days. Researchers used AlphaFold and the drug discovery platform Pharma.AI to design and synthesize a potential drug to treat hepatocellular carcinoma (HCC), the most common type of primary liver cancer. \n \n And so much more (every day)... it’s breathtaking! \n The CEO of Stability AI Emad Mostaque, who I’ve had on my podcast and on the Abundance360 stage, recently made an extraordinary point. \n “Google and Microsoft are going all-in with generative AI as core to their future. There is no ‘we are still early’ here, trillion-dollar companies are shifting their whole strategy and focus.” \n -- Emad Mostaque, CEO, Stability AI \n And we’re seeing this play out. \n Microsoft invested $10 billion in OpenAI earlier this year and Bing (remember Bing?) is now AI-powered and disrupting online search. Bing now has over 100 million daily active users. \n Google has been on red alert for months to defend its search business against Microsoft, OpenAI, and others. The company released its own AI-powered chatbot Bard earlier this year, and just last week made two key announcements. First, Google is creating new AI-powered search tools as part of its project “Magi.” Second, the company is consolidating all of its AI research divisions into Google DeepMind. \n Meanwhile, Elon has announced a new AI company called X.ai to rival OpenAI. \n And Stability AI itself just released its suite of open-source large language models (LLMs), ranging from 3 billion to 7 billion parameters. \n Why This Matters: \n Alphabet CEO Sundar Pichai has said, “Artificial intelligence could have more profound implications for humanity than electricity or fire.” \n I agree. The way I put it, perhaps more bluntly, is: \n “ By the end of this decade there are going to be two kinds of companies: those that are fully utilizing AI and those that are out of business.” \n The choice is yours. \n In the coming years, it will be the entrepreneurs, creators, and leaders who work with technology that will have the most impact. \n We have an enormous opportunity to use AI for good. \n What will you create? How will YOU uplift humanity? \n This blog is the first in a new series about AI. In our next blog on the topic, we'll cover the Ethics of AI . \n NOTE : I'm hosting a four-hour Workshop on Generative AI next month as part of my year-round Abundance360 leadership program. If you're interested in participating in the Workshop and learning more about Abundance360, click here .",
        "image": "https://www.diamandis.com/hubfs/green-black-lines-glass-sphere.jpeg",
        "favicon": "https://www.diamandis.com/hubfs/PHD_favicon.jpg"
      },
      {
        "score": 0.17056244611740112,
        "title": "Waifu Labs - Welcome to Waifu Labs v2: How do AIs Create?",
        "id": "https://waifulabs.com/blog/ai-creativity",
        "url": "https://waifulabs.com/blog/ai-creativity",
        "publishedDate": "2022-01-07T00:00:00.000Z",
        "author": "",
        "text": "If you haven't already had a chance to try out the new Waifulabs v2 Model, check it out now! \n \nIn the two years since we first launched our our anime portraits AI project, Waifu Labs, our inhuman artist has been hard at work. Recently, it finished drawing its 20-millionth commission (quite an achievement for any artist!) \n \nTo count down the release of Arrowmancer , our mobile game illustrated using AI, we’re releasing a series of posts addressing the topic of artificial creativity. \n \nStarting with the number one question: how do AIs (or even humans for that matter) create? \n \n \nHow Does it Work? \n \n \n \n(Left: training progress, Right: the finished result) \n \nThe type of AI that powers Waifu Labs is called a Generative Adversarial Network . We made an explainer video on how the technology works! \n \n \n \nYou can think of it as a pair of AIs that spar against each other in order to learn: \n \n \nThe first AI is called the Generator . It’s task is to learn how to draw. \n \nThe second AI is called the Discriminator . It’s task is to learn how to tell fake drawings (produced by an AI) from real drawings (produced by a human artist). \n \n \nBoth AIs are exposed to anime data from human artists and offered feedback on how they performed on their respective tasks. At the very end, we separate out the Generator and run it as the artist behind the scenes. \n \nIt is interesting to note that like human rivals, it is imperative that they grow at the same rate. When one AI dominates the other, both stop learning. \n \nBelow, you can see the Generator’s progress: we’ve collected samples of the work that it produces as it trains. We’ve measured the training time in “steps.” \n \n Step 0 : You can see that the AI starts off with absolutely no idea what “art” is. The first picture it draws is noise, a random splash of primordial test-tube goop. It will bumble around like this for a while, trying out strategies to draw more like the human art that it sees. \n \n Step 1024 : We see the beginnings of a face come in. \n \n Step 4096 : Slightly style-specific features, anime-like strands of hair, start to emerge \n \n Step 13516 : We find that, just like human artists, the AI always learns how to draw the eyes first. \n \n Step 23961 : Secondary features like ears and shoulders start to come in. \n \n Step 40564 : Gradually, the murky shapes resolve into real features. As the size of the features get smaller and smaller, they give the illustration its “texture” and “style.” Color will be the last to come in. \n \n Step 43636 : During this phase, the training gets unstable at times, so we have snapshots of occasional horrors like this. \n \n Step 50000 : The final result! \n \nFor this particular AI, we are able to parallelize the training by building a mini-supercomputer in our living room from scratch to accommodate its specific needs — though that merits its own story another time. \n \nAll in all, the Waifu Labs AI took about 2 weeks to reach the quality you see today. \n \nBuilding a Mental Representation \n \nIt is interesting to note that from this process, the AI is not merely learning to copy the works it has seen, but forming high-level (shapes) and low-level (texture) features for constructing original pictures in its own mental representation. \n \nHuman artists also have this mental representation in their meat-brains. For many artists, this representation lives beyond the realm of conscious thought in the domain of intuition. This mental representation grows throughout the artist’s lifetime: it matures as a combination of practicing art and the accumulating life experience on the meat-body. \n \nWith an AI, the mental representation is locked the moment the training stops, so we can then take it out and peer directly at it. It looks a bit like a coordinate system, and we call this the latent space. \n \nBy isolating the vectors that control certain features, we can create results like \n \ndifferent pose, same character: \n \n \nsame pose, different style: \n \n \nAnd much more! \n \nSome coordinates in latent space are definitely weird. Our favorite is this gal, affectionately nicknamed “bighead.” \n \n \n \nLatent space is immeasurably huge. Parched. Devoid of content. We wade through vast deserts of average artwork to find the coordinates for alluring, captivating characters. Cataloging these coordinates allows us to do cool tricks, like morph between characters. \n \nYou can also see the latent space mapping in action on Waifu Labs: by separating the coordinates that control pose, colors, and details, we can create an interface where humans can commission the AI artist to draw what they want! \n \nHow do you measure creativity? \n \nA large part of developing the AI is the evaluation of performance, which brings up the philisophical argument of how do you evaluate creativity? Conventionally, FID is the score used to score generative AIs, but this was not quite sufficient for our needs. \n \nTo properly score the results of our AI artist, we took a rubric from another domain. Namely, the qualitative rubric that commercial art directors use to evaluate freelance artists. \n \n \n Quality : The artist can match the requested style (anime portraits) \n \n Diversity : The artist can depict a variety of traits in the requested Quality \n \n Customizability : It is easy to select the specific traits from the space that the artist is able to depict for a specific portrait \n \n Time : The artist can deliver the piece in a timely manner \n \n \nSome interesting points to note: \n \n \nQuality is defined as conformity to a style. As far as commercial art direction is concerned, arrangement is key: having beautiful art pieces of different styles doesn’t work as well as having less impressive pieces that complement and match each other. \n \nCustomizability is an interesting topic for human artists. This is often referred to in the industry as “professional manner”: the ability to communicate and figure out what the client wants. It’s not necessarily a function of pure art skill as much as it is writing good emails. Similarly, for our machine artist, this is the metric by which we evaluated the UI interaction. \n \n \nClosing thoughts \n \nOf course, it’s not sufficient to define the sum of creativity by its usefulness in commercial application. However, using these metrics, we were able to reach a good proxy and make an AI formats its outputs in a way that can drive the core creative design and production pipeline of a commercial product. \n \nThe creative industry and the act of creativity are two different, deeply intertwined concepts. Though we may never breatch the mystiques of the later, we can peer at it through the lens of the former. \n \nTo that end, please take a moment to check out the commercial product in action, our mobile game, Arrowmancer is made using the AI behind Waifu Labs! \n \nWhen we first approached studios with our AI work, the primary response we received was bewilderment. While we could fulfill the above requirements for freelance illustration, there was no existing project that made use of the concept of infinitely customizable, combineable illustration. \n \nAnd so we set out to do it ourselves: we made a game that leverages the the unique aspects of generative AI. Is is our hope it will usher in a new age of creativity, a collaborative model of asset production between human and machine creatives!",
        "image": "https://waifulabs.com/assets/blog/ai-creativity/GAN_thumbnail_00000.png",
        "favicon": "https://waifulabs.com/images/favicon.png"
      },
      {
        "score": 0.16962090134620667,
        "title": "Designing Intelligent Artificial Intelligence",
        "id": "https://www.gamedev.net/blogs/entry/2263283-designing-intelligent-artificial-intelligence/",
        "url": "https://www.gamedev.net/blogs/entry/2263283-designing-intelligent-artificial-intelligence/",
        "publishedDate": "2023-02-06T07:35:22.000Z",
        "author": "Eric Nevala",
        "text": "Below is my preliminary draft design for the AI system within Spellbound. I'm slowly migrating away from scripted expert systems towards a more dynamic and fluid AI system based on machine learning and neural networks. I may be crazy to attempt this, but I find this topic fascinating. I ended up having a mild existential crisis as a result of this. Let me know what you think or if I'm missing something. \n \n \n Artificial Intelligence: \n \n \n Objectives: \nSpellbound is going to be a large open world with many different types of characters, each with different motives and behaviors. We want this open world to feel alive, as if the characters within the world are inhabitants. If we went with pre-scripted behavioral patterns, the characters would be unable to learn and adapt to changes in their environment. It would also be very labor intensive to write specific AI routines for each character. Ideally, we just give every character a self-adapting brain and let them loose to figure out the rest for themselves.\n \n \n Core Premise: (very dense, take a minute to soak this in) \nIntelligence is not a fixed intrinsic property of creatures. Intelligence is an emergent property which results directly from the neural topology of a biological brain. True sentience can be created if the neural topology of an intelligent being is replicated with data structures and the correct intelligence model. If intelligence is an emergent property, and emergent properties are simple rule sets working together, then creating intelligence is a matter of discovering the simple rule sets.\n \n \n Design: \nEach character has its own individual Artificial Neural Network (ANN). This is a weighted graph which uses reinforcement learning. Throughout the character's lifespan, the graph will become more weighted towards rewarding actions and away from displeasurable ones. Any time an action causes a displeasure to go away or brings a pleasure, that neural pathway will be reinforced. If a neural pathway has not been used in a long time, we reduce its weight. Over time, the creature will learn.\n \n \nA SIMPLE ANN is just a single cluster of connected neurons. Each neuron is a “node” which is connected to nearby neurons. Each neuron receives inputs and generates outputs. The neural outputs always fire and activate a connected neuron. When a neuron receives enough inputs, it itself fires and activates downstream neurons. So, a SIMPLE ANN receives input and generates outputs which are a reaction to the inputs. At the end of neural cycle, we have to give response feedback to the ANN. If the neural response was positive, we strengthen the neural pathway by increasing the neural connection weights. If the response was negative, we decrease the weights of the pathway. With enough trial runs, we will find the neural pathway for the given inputs which creates the most positive outcome.\n \n \nThe SIMPLE ANN can be considered a single cluster. It can be abstracted into a single node for the purposes of creating a higher layer of connected node networks. When we have multiple source inputs feeding into our neural network cluster and each node is running its most optimal neural pathway depending on the input, we get complex unscripted behavior. A brain is just a very large collection of layered neural nodes connected to each other. We’ll call this our “Artificial Brain” (AB)\n \n \n Motivation, motivators (rule sets): \n-All creatures have a “desired state” they want to achieve and maintain. Think about food. When you have eaten and are full, your state is at an optimally desired state. When time passes, you become increasingly hungry. Being just a teensy bit hungry may not be enough to compel you to change your current behavior, but as time goes on and your hunger increases, your motivation to eat increases until it supersedes the motives for all other actions. We can create a few very simple rules to create complex, emergent behavior. \nRule 1: Every creature has a desired state they are trying to achieve and maintain. Some desired states may be unachievable (ie, infinite wealth) \nRule 2: States are changed by performing actions. Actions may change one or more states at once (one to many relationship). \nRule 3: “Motive” is created by a delta between current state (CS) and desired state (DS). The greater the delta between CS and DS, the more powerful the motive is. (Is this a linear graph or an exponential graph?) \nRule 4: “relief” is the sum of all deltas between CS and DS provided by an action. \nRule 5: A creature can have multiple competing motives. The creature will choose the action which provides the greatest amount of relief. \nRule 6: Some actions are a means to an end and can be chained together (action chains). If you’re hungry and the food is 50 feet away from you, you can’t just start eating. You first must move to the food to get within interaction radius, then eat it.\n \n \nQ: How do we create an action chain? \nQ: How do we know that the action chain will result in relief? \nA: We generally know what desired result we want, so we work backwards. What action causes desired result (DR)? Action G does (learned from experience). How do we perform Action G? We have to perform Action D, which causes Action G. How do we cause Action D? We perform Action A, which causes Action D. Therefore, G&lt;-D&lt;-A; So we should do A-&gt;D-&gt;G-&gt;DR. Back propagation may be the contemporary approach to changing graph weights, but it's backwards. \nQ: How does long term planning work? \nQ: What is a conceptual idea? How can it be represented? \nA: A conceptual idea is a set of nodes which is abstracted to become a single node?\n \n \n Motivators: (Why we do the things we do) \nHunger \nBody Temperature \nWealth \nKnowledge \nPower \nSocial Validation \nSex \nLove/Compassion \nAnger/Hatred \nPain Relief \nFear \nVirtues, Vices &amp; Ethics \nNotice that all of these motivators are actually psychological motivators. That means they happen in the head of the agent rather than being a physical motivator. You can be physically hungry, but psychologically, you can ignore the pains of hunger. The psychological thresholds would be different per agent. Therefore, all of these motivators belong in the “brain” of the character rather than all being attributes of an agents physical body. Hunger and body temperature would be physical attributes, but they would also be “psychological tolerances”.\n \n \n Psychological Tolerances: \n \n \n{motivator} =&gt; 0 [------------|-----------o----|----] 100\nA B C D E \n \nA - This is the lowest possible bound for the motivator. \nB - This is the lower threshold point for the motivator. If the current state falls below this value, the desired state begins to affect actions. \nC - This is the current state of the motivator. \nD - This is the upper threshold point for the motivator. If the current state exceeds this value, the desired state begins to affect actions. \nE - This is the highest bounds for the motivator.\n \n \nThe A &amp; E bounds values are fixed and universal. \nThe B and D threshold values vary by creature. Where you place them can make huge differences in behavior.\n \n \n Psychological Profiles: \nWe can assign a class of creatures a list of psychological tolerances and assign their current state to some preset values. The behavioral decisions and subsequent actions will be driven by the psychological profile based upon the actions which create the sum of most psychological relief. The psychological profile will be the inputs into an artificial neural network, and the outputs will be the range of actions which can be performed by the agent. Ideally, the psychological profile state will drive the ANN, which drives actions, which changes the state of the psychological profile, which creates a feedback loop of reinforcement learning. \n \n \n Final Result: \nWe do not program scripted behaviors, we assign psychological profiles and lists of actions. Characters will have psychological states which drive their behavioral patterns. Simply by tweaking the psychological desires of a creature, we can create emergent behavior resembling intelligence. A zombie would always be hungry, feasting on flesh would provide temporary relief. A goblin would have a strong compulsion for wealth, so they'd be very motivated to perform actions which ultimately result in gold. Rather than spending lots of time writing expert systems styled AI, we create a machine learning type of AI.\n \n \n Challenges: \nI have never created a working artificial neural network type of AI. \n \n \n Experimental research and development: \n \n \n The following notes are crazy talk which may or may not be feasible. They may need more investigation to measure their merit as viable approaches to AI. Learning by Observation: \nOur intelligent character doesn’t necessarily have to perform an action themselves to learn about its consequences (reward vs regret). If they watch another character perform an action and receive a reward, the intelligent character creates a connection between an action and consequence. \n Exploration Learning: \nA very important component to getting an simple ANN to work most efficiently is to get the neurons to find and establish new connections with other neurons. If we have a neural connection topology which always results in a negative response, we’ll want to generate a new connection at random to a nearby neuron.\n \n \n Exploration Scheduling: \nWhen all other paths are terrible, the new path becomes better and we “try it out” because there’s nothing better. If the new pathway happens to result in a positive outcome, suddenly it gets much stronger. This is how our simple ANN discovers new unscripted behaviors.\n \n \nThe danger is that we will have a sub-optimal behavior pattern which generates some results, but they’re not the best results. We’d use the same neural pathway over and over again because it is a well travelled path.\n \n \n Exploration Rewards: \nIn order to encourage exploring different untravelled paths, we gradually increase the “novelty” reward value for taking that pathway. If traveling this pathway results in a large reward, the pathway is highly rewarded and may become the most travelled path.\n \n \n Dynamic Deep Learning: \nOn occasion, we’ll also want to create new neurons at random and connect them to at least one other nearby downstream neuron. If a neuron is not connected to any other neurons, it becomes an “island” and must die. When we follow a neural pathway, we are looking at two costs: The connection weight and the path weight. We always choose the shortest path with the least weight. Rarely used pathways will have their weight decrease over a long period of time. If a path weight reaches zero, we break the connection and our brain “forgets” the neural connection. \n Evolutionary &amp; Inherited Learning: \nIt takes a lot of effort for a neural pathway to become developed. We will want to speed up the development. If a child is born to two parents, those parents will rapidly increase the neural pathways of the child by sharing their own pathways. This is one way to \"teach\". Thus, children will think very much like their parents do. Other characters will also share their knowledge with other characters. In order for knowledge to spread, it must be interesting enough to be spread. So, a character will generally share the most interesting knowledge they have.\n \n \n Network Training &amp; Evolutionary Inheritance: \nAn untrained ANN results in an uninteresting character. So, we have to have at least a trained base preset for a brain. This is consistent with biological brains because our brains have been pre-configured through evolutionary processes and come pre-wired with certain regions of the brain being universally responsible for processing certain input types. The training method will be rudimentary at first, to get something at least passable, and it can be done as a part of the development process. \nWhen we release the game to the public, the creatures are still going to be training. The creatures which had the most “success” will become a part of the next generation. These brain configurations can be stored on a central database somewhere in the cloud. When a player begins a new game, we download the most recent generation of brain configurations. Each newly instanced character may have a chance to have a random mutation. When the game completes, if there were any particular brains which were more successful than the current strain, we select it for “breeding” with other successful strains so that the next generation is an amalgamation of the most successful previous generations. We’ll probably begin to see some divergence and brain species over time?\n \n \n Predisposition towards Behavior Patterns via bias: \nCharacters will also have slight predispositions which are assigned at birth. 50% of their predisposition is innate to their creature class. 25% is genetically passed down by parents. 25% is randomly chosen. A predisposition causes some pleasures and displeasures to be more or less intense. This will skew the weightings of a developing ANN a bit more heavily to favor particular actions. This is what will create a variety in interests between characters, and will ultimately lead to a variety in personalities. We can create very different behavior patterns in our AB’s by tweaking the amount of pleasure and displeasure various outputs generate for our creature. The brain of a goblin could derive much more pleasure from getting gold, so it will have strong neural pathways which result in getting gold.\n \n \nAI will be able to interact with interactable objects. An interactable object has a list of ways it can be interacted with. Interactable objects can be used to interact with other interactable objects. Characters are considered to be interactable objects. The AI has a sense of ownership for various objects. When it loses an object, it is a displeasurable feeling. When they gain an object, it is a pleasurable feeling. Stealing from an AI will cause it to be unhappy and it will learn about theft and begin trying to avoid it. Giving a gift to an AI makes it very happy. Trading one object for another will transfer ownership of objects. There is no \"intrinsic value\" to an object. The value of an object is based on how much the AI wants it compared to how much it wants the other object in question. \n Learning through Socialization: \nAI's will socialize with each other. This is the primary mechanism for knowledge transfer. They will generally tell each other about recent events or interests, choosing to talk about the most interesting events first. If an AI doesn't find a conversation very interesting, they will stop the conversation and leave (terminating condition). If a threat is nearby, the AI will be very interested in it and will share with nearby AI. If a player has hurt or killed a townsfolk, all of the nearby townsfolk will be very upset and may attack the player on sight. If enough players attack the townsfolk, the townsfolk AI will start to associate all players with negative feelings and may attack a player on sight even if they didn't do anything to aggravate the townsfolk AI. \n ||||I|||| All Content Blogs Forums News Tutorials\nLog In\nSign Up\n \nLogin\nUsername / Email\nPassword\nRemember me\nForgot password?\nLogin\nor\nDon't have a GameDev.net account? Sign up\n \nForgot your password?\nEmail Address\nReset Password\nPlease contact us if you have any trouble resetting your password.\nHome\nBlogs\nCareers\n* Careers\nForums\nNews\nPortfolios\nProjects\nTutorials\nNew? Learn about game development\nFollow Us\nChat in the GameDev.net Discord!\nMastodon\nDesigning Intelligent Artificial Intelligence\nposted in slayemin's Journal\nPublished July 19, 2017\nBehavior MachineLearning NeuralNetworks\nAdvertisement\nBelow is my preliminary draft design for the AI system within Spellbound. I'm slowly migrating away from scripted expert systems towards a more dynamic and fluid AI system based on machine learning and neural networks. I may be crazy to attempt this, but I find this topic fascinating. I ended up having a mild existential crisis as a result of this. Let me know what you think or if I'm missing something.\nArtificial Intelligence:\nObjectives:\nSpellbound is going to be a large open world with many different types of characters, each with different motives and behaviors. We want this open world to feel alive, as if the characters within the world are inhabitants. If we went with pre-scripted behavioral patterns, the characters would be unable to learn and adapt to changes in their environment. It would also be very labor intensive to write specific AI routines for each character. Ideally, we just give every character a self-adapting brain and let them loose to figure out the rest for themselves.\nCore Premise: (very dense, take a minute to soak this in)\nIntelligence is not a fixed intrinsic property of creatures. Intelligence is an emergent property which results directly from the neural topology of a biological brain. True sentience can be created if the neural topology of an intelligent being is replicated with data structures and the correct intelligence model. If intelligence is an emergent property, and emergent properties are simple rule sets working together, then creating intelligence is a matter of discovering the simple rule sets.\nDesign:\nEach character has its own individual Artificial Neural Network (ANN). This is a weighted graph which uses reinforcement learning. Throughout the character's lifespan, the graph will become more weighted towards rewarding actions and away from displeasurable ones. Any time an action causes a displeasure to go away or brings a pleasure, that neural pathway will be reinforced. If a neural pathway has not been used in a long time, we reduce its weight. Over time, the creature will learn.\nA SIMPLE ANN is just a single cluster of connected neurons. Each neuron is a “node” which is connected to nearby neurons. Each neuron receives inputs and generates outputs. The neural outputs always fire and activate a connected neuron. When a neuron receives enough inputs, it itself fires and activates downstream neurons. So, a SIMPLE ANN receives input and generates outputs which are a reaction to the inputs. At the end of neural cycle, we have to give response feedback to the ANN. If the neural response was positive, we strengthen the neural pathway by increasing the neural connection weights. If the response was negative, we decrease the weights of the pathway. With enough trial runs, we will find the neural pathway for the given inputs which creates the most positive outcome.\nThe SIMPLE ANN can be considered a single cluster. It can be abstracted into a single node for the purposes of creating a higher layer of connected node networks. When we have multiple source inputs feeding into our neural network cluster and each node is running its most optimal neural pathway depending on the input, we get complex unscripted behavior. A brain is just a very large collection of layered neural nodes connected to each other. We’ll call this our “Artificial Brain” (AB)\nMotivation, motivators (rule sets):\n-All creatures have a “desired state” they want to achieve and maintain. Think about food. When you have eaten and are full, your state is at an optimally desired state. When time passes, you become increasingly hungry. Being just a teensy bit hungry may not be enough to compel you to change your current behavior, but as time goes on and your hunger increases, your motivation to eat increases until it supersedes the motives for all other actions. We can create a few very simple rules to create complex, emergent behavior.\nRule 1: Every creature has a desired state they are trying to achieve and maintain. Some desired states may be unachievable (ie, infinite wealth)\nRule 2: States are changed by performing actions. Actions may change one or more states at once (one to many relationship).\nRule 3: “Motive” is created by a delta between current state (CS) and desired state (DS). The greater the delta between CS and DS, the more powerful the motive is. (Is this a linear graph or an exponential graph?)\nRule 4: “relief” is the sum of all deltas between CS and DS provided by an action.\nRule 5: A creature can have multiple competing motives. The creature will choose the action which provides the greatest amount of relief.\nRule 6: Some actions are a means to an end and can be chained together (action chains). If you’re hungry and the food is 50 feet away from you, you can’t just start eating. You first must move to the food to get within interaction radius, then eat it.\nQ: How do we create an action chain?\nQ: How do we know that the action chain will result in relief?\nA: We generally know what desired result we want, so we work backwards. What action causes desired result (DR)? Action G does (learned from experience). How do we perform Action G? We have to perform Action D, which causes Action G. How do we cause Action D? We perform Action A, which causes Action D. Therefore, G D->G->DR. Back propagation may be the contemporary approach to changing graph weights, but it's backwards.\nQ: How does long term planning work?\nQ: What is a conceptual idea? How can it be represented?\nA: A conceptual idea is a set of nodes which is abstracted to become a single node?\nMotivators: (Why we do the things we do)\nHunger\nBody Temperature\nWealth\nKnowledge\nPower\nSocial Validation\nSex\nLove/Compassion\nAnger/Hatred\nPain Relief\nFear\nVirtues, Vices & Ethics\nNotice that all of these motivators are actually psychological motivators. That means they happen in the head of the agent rather than being a physical motivator. You can be physically hungry, but psychologically, you can ignore the pains of hunger. The psychological thresholds would be different per agent. Therefore, all of these motivators belong in the “brain” of the character rather than all being attributes of an agents physical body. Hunger and body temperature would be physical attributes, but they would also be “psychological tolerances”.\nPsychological Tolerances:\n{motivator} => 0 [------------|-----------o----|----] 100\nA B C D E\nA - This is the lowest possible bound for the motivator.\nB - This is the lower threshold point for the motivator. If the current state falls below this value, the desired state begins to affect actions.\nC - This is the current state of the motivator.\nD - This is the upper threshold point for the motivator. If the current state exceeds this value, the desired state begins to affect actions.\nE - This is the highest bounds for the motivator.\nThe A & E bounds values are fixed and universal.\nThe B and D threshold values vary by creature. Where you place them can make huge differences in behavior.\nPsychological Profiles:\nWe can assign a class of creatures a list of psychological tolerances and assign their current state to some preset values. The behavioral decisions and subsequent actions will be driven by the psychological profile based upon the actions which create the sum of most psychological relief. The psychological profile will be the inputs into an artificial neural network, and the outputs will be the range of actions which can be performed by the agent. Ideally, the psychological profile state will drive the ANN, which drives actions, which changes the state of the psychological profile, which creates a feedback loop of reinforcement learning.\nFinal Result:\nWe do not program scripted behaviors, we assign psychological profiles and lists of actions. Characters will have psychological states which drive their behavioral patterns. Simply by tweaking the psychological desires of a creature, we can create emergent behavior resembling intelligence. A zombie would always be hungry, feasting on flesh would provide temporary relief. A goblin would have a strong compulsion for wealth, so they'd be very motivated to perform actions which ultimately result in gold. Rather than spending lots of time writing expert systems styled AI, we create a machine learning type of AI.\nChallenges:\nI have never created a working artificial neural network type of AI.\nExperimental research and development:\nThe following notes are crazy talk which may or may not be feasible. They may need more investigation to measure their merit as viable approaches to AI.\nLearning by Observation:\nOur intelligent character doesn’t necessarily have to perform an action themselves to learn about its consequences (reward vs regret). If they watch another character perform an action and receive a reward, the intelligent character creates a connection between an action and consequence.\nExploration Learning:\nA very important component to getting an simple ANN to work most efficiently is to get the neurons to find and establish new connections with other neurons. If we have a neural connection topology which always results in a negative response, we’ll want to generate a new connection at random to a nearby neuron.\nExploration Scheduling:\nWhen all other paths are terrible, the new path becomes better and we “try it out” because there’s nothing better. If the new pathway happens to result in a positive outcome, suddenly it gets much stronger. This is how our simple ANN discovers new unscripted behaviors.\nThe danger is that we will have a sub-optimal behavior pattern which generates some results, but they’re not the best results. We’d use the same neural pathway over and over again because it is a well travelled path.\nExploration Rewards:\nIn order to encourage exploring different untravelled paths, we gradually increase the “novelty” reward value for taking that pathway. If traveling this pathway results in a large reward, the pathway is highly rewarded and may become the most travelled path.\nDynamic Deep Learning:\nOn occasion, we’ll also want to create new neurons at random and connect them to at least one other nearby downstream neuron. If a neuron is not connected to any other neurons, it becomes an “island” and must die. When we follow a neural pathway, we are looking at two costs: The connection weight and the path weight. We always choose the shortest path with the least weight. Rarely used pathways will have their weight decrease over a long period of time. If a path weight reaches zero, we break the connection and our brain “forgets” the neural connection.\nEvolutionary & Inherited Learning:\nIt takes a lot of effort for a neural pathway to become developed. We will want to speed up the development. If a child is born to two parents, those parents will rapidly increase the neural pathways of the child by sharing their own pathways. This is one way to \"teach\". Thus, children will think very much like their parents do. Other characters will also share their knowledge with other characters. In order for knowledge to spread, it must be interesting enough to be spread. So, a character will generally share the most interesting knowledge they have.\nNetwork Training & Evolutionary Inheritance:\nAn untrained ANN results in an uninteresting character. So, we have to have at least a trained base preset for a brain. This is consistent with biological brains because our brains have been pre-configured through evolutionary processes and come pre-wired with certain regions of the brain being universally responsible for processing certain input types. The training method will be rudimentary at first, to get something at least passable, and it can be done as a part of the development process.\nWhen we release the game to the public, the creatures are still going to be training. The creatures which had the most “success” will become a part of the next generation. These brain configurations can be stored on a central database somewhere in the cloud. When a player begins a new game, we download the most recent generation of brain configurations. Each newly instanced character may have a chance to have a random mutation. When the game completes, if there were any particular brains which were more successful than the current strain, we select it for “breeding” with other successful strains so that the next generation is an amalgamation of the most successful previous generations. We’ll probably begin to see some divergence and brain species over time?\nPredisposition towards Behavior Patterns via bias:\nCharacters will also have slight predispositions which are assigned at birth. 50% of their predisposition is innate to their creature class. 25% is genetically passed down by parents. 25% is randomly chosen. A predisposition causes some pleasures and displeasures to be more or less intense. This will skew the weightings of a developing ANN a bit more heavily to favor particular actions. This is what will create a variety in interests between characters, and will ultimately lead to a variety in personalities. We can create very different behavior patterns in our AB’s by tweaking the amount of pleasure and displeasure various outputs generate for our creature. The brain of a goblin could derive much more pleasure from getting gold, so it will have strong neural pathways which result in getting gold.\nAI will be able to interact with interactable objects. An interactable object has a list of ways it can be interacted with. Interactable objects can be used to interact with other interactable objects. Characters are considered to be interactable objects. The AI has a sense of ownership for various objects. When it loses an object, it is a displeasurable feeling. When they gain an object, it is a pleasurable feeling. Stealing from an AI will cause it to be unhappy and it will learn about theft and begin trying to avoid it. Giving a gift to an AI makes it very happy. Trading one object for another will transfer ownership of objects. There is no \"intrinsic value\" to an object. The value of an object is based on how much the AI wants it compared to how much it wants the other object in question.\nLearning through Socialization:\nAI's will socialize with each other. This is the primary mechanism for knowledge transfer. They will generally tell each other about recent events or interests, choosing to talk about the most interesting events first. If an AI doesn't find a conversation very interesting, they will stop the conversation and leave (terminating condition). If a threat is nearby, the AI will be very interested in it and will share with nearby AI. If a player has hurt or killed a townsfolk, all of the nearby townsfolk will be very upset and may attack the player on sight. If enough players attack the townsfolk, the townsfolk AI will start to associate all players with negative feelings and may attack a player on sight even if they didn't do anything to aggravate the townsfolk AI.\nCancel Save\nPrevious Entry Spellbound: May-July Updates\nNext Entry July/August Update\n4 likes 4 comments\nShare:\nLatest Comments\nkseh\nWhat is it that you're hoping that an AI like this adds to Spellbound?\nThe mindless undead that attack the player... well they're mindless, so they don't really need an AI, right? For an AI like this to be applied to an end boss or other main character that may or may not be adversarial, I would think that you would run the risk of not providing a consistent experience for players when the boss is encountered. Which then would leave assorted NPCs like villagers to be given the AI, the main purpose of which, from a game play perspective, I would think would be to generate side quests which I would expect there to be simpler ways to go about generating.\nCancel Save\nJuly 24, 2017 06:48 PM\nSize_A5\nInteresting.\nYou've managed to reinvent several concepts, including drive reduction theory, mirror neurons (sort of), neural Darwinism, and NEAT (NeuroEvolution through Augmenting Topology). I'm quite impressed.\nWith that having been said, I'm afraid kseh is correct. Adding neural net-based AI of this kind of complexity wouldn't contribute much to your game. Additionally, depending on how enemies are processed, the size of each neural net, and the specific implementation, you may find that using this kind of AI will simply bog down the user's computer compared to the relatively computationally cheap expert system.\nAI stuff is super fun to geek out about, as you've seen, but much of it is also very impractical. In your previous blog post, you had concerns over whether anyone would care. I think the answer is that AI enthusiasts might care, but your players won't. There's an article written by some Halo developers on Halo's AI. They originally had an elaborate AI scheme utilizing fuzzy logic, but playtesting found that players consistently failed to notice and/or take advantage of the NPC's behaviors.\nI think you're on the right track with abstracting actions into \"abilities\" to simplify AI development. For more intelligent enemies, using some system to establish preferences seems like a good idea, but I would abstain from neural nets. Perhaps you should use an expert system template and just define preference values for each enemy type?\nCancel Save\nJuly 24, 2017 11:33 PM\nslayemin\nYeah, I'm a novice at AI and have not spent a lot of time studying it formally. That's probably why I reinvent AI concepts familiar to AI developers. Currently, my developer attitude is, \"What does it take to ship right now?\" mixed with \"How do I avoid painting myself into a corner?\"\nI'm currently modifying my expert system to use abilities, but structuring my abilities system to be something that can be treated as nodes in a graph network if I ever want to transition to an ANN. The underlying reasoning for this is that eventually my list of characters is going to be pretty large and complicated, and as I add in more characters, the scope and complexity increases. I'll need to have a strategy for reducing the developer work load and being able to adapt behaviors to game design changes without completely refactoring my expert systems AI code. What I wrote above is a rough outline for a direction I can eventually go in.\nI'm thinking that this may be a bit of a waste of time right now, but I've convinced myself that there is something truly magical about having the illusion of an intelligent creature interacting with you in virtual reality. A part of that magic comes from being surprised by the actions and behaviors of a creature. The less scripted and novel the behavior seems, the more amazing it is. If eventually we have lots of AI systems doing complex behavior to \"live\" in the virtual world and the players actions are a big contributing factor in the behavior of the world characters, then the replay value and player engagement increases by several orders of magnitude. Players can have really different game play experiences when they do a \"good\" play through vs. \"evil\" play through, and everything in between. I think the variety in consequences within the game makes the moral choices really interesting and becomes a way for players to explore their own nature/hearts within a consequence free world, and then they take those learned lessons back to real life. A flexible/adaptive AI system would be a necessary component to exploring the long term consequences of moral decisions within the framework of a game. Hopefully, the end result would be that virtuous actions are always better.\nCancel Save\nJuly 25, 2017 02:48 AM\njackmorris\nArtificial Intelligence is very interesting phenomenon and it makes the future more technological. At class we discussed its advantages and disadvantages and with the help of this site https://writingbros.com/essay-examples/artificial-intelligence/ I was able to prove my point and bring very objective arguments. I think that using AI during the design could bring a lot of success to its creators.\nCancel Save\nJanuary 30, 2022 09:24 PM\nYou must log in to join the discussion.\nDon't have an account? Sign up!\nslayemin\nAuthor\nFollow\nAdvertisement\nLatest Entries\nThree months in at Electronic Arts\nOctober 07, 2022\nProgrammer Art & Digital Humans\nOctober 09, 2020\nMoved my office today\nSeptember 22, 2019\nI'm quite happy\nAugust 02, 2019\nThe only constant in life is change.\nJuly 09, 2019\nAce Pilot Game\nFebruary 20, 2019\nA wild thought appeared\nJanuary 22, 2019\nShipped a pilot\nDecember 13, 2018\nKissing rock bottom\nNovember 21, 2018\nArtificial Intelligence Work in Progress\nOctober 13, 2018\nSee all entries\nAdvertisement\nReticulating splines\nAbout GameDev.net\nTerms of Service\nPrivacy Policy\nContact Us\nCopyright (c) 1999-2023 GameDev.net, LLC\nBack to Top"
      },
      {
        "score": 0.1683480441570282,
        "title": "Why AI is a Lie & Everyone Knows it - Towards Data Science",
        "id": "https://towardsdatascience.com/why-ai-is-a-lie-everyone-knows-it-92e200fc900b?gi=d76b00f439d3",
        "url": "https://towardsdatascience.com/why-ai-is-a-lie-everyone-knows-it-92e200fc900b?gi=d76b00f439d3",
        "publishedDate": "2019-10-29T18:21:08.000Z",
        "author": "Peter Salinas",
        "text": "Photo by Ben White on Unsplash Time to lose some old friends and make some new ones! First, take a moment to distill and define what intelligence is. OK. How many of you agree with that definition? There is probably a massive divide. In the “Academic” community I belong to, which not a lot of people like, we call this “Bias”. Its very simple, if something cannot be measured and understood “universally”, relative to whatever social universe you exist in its “Theory”. Ok. What is “Artificial”? Well, this one is fairly simple. So, we at least know one part about AI is universally true, its all not real, I mean… Artificial. Zing. We are hyped on something we can’t universally define that isn’t real. Sorta. You're Intelligence no matter how you look at it Our brains consume patterns, effectively, they consume math, day one. Day one is also something arguable today, sadly, the few groups that had some footing researching what “Day 1” for babies is, got squashed pretty early because of a LOT of cultural reasons and “Big Business” of academia knows you can’t sell well on culturally touchy topics of the time. What was learned, however, is that we can effectively translate patterns very young. Our “Research” in some form during a similar time demonstrated “Classical Music makes your baby smarter”. Sorta. Contextually speaking. What wasn’t clarified is its less about the music, its more the consistency of patterns. And we use those patterns to trigger how we react today. There is a good chance if you’re very musically inclined, artistic, or use music in any way to trigger some process of thinking, you had very specific patterns of music exposed to you very young. In many ways, this is a topic of “AI”. It's about “Learning”. Brains in infancy, Artificial or Organic, will grow relative to the patterns of data that are being digested. You have to have things train those brains, but you also have to have some kind of trajectory or output of where you want that training to go. There are lots of little details here that I can save for another post, but there is quite a bit of beautiful and terrifying math to it all that could make things get weird. Suffice to say, all things we grow are in many ways an abstraction of ourselves. You can call it a generational curse, legacy, lineage, social learning, neural networks, choose your cultural bias. Math doesn’t care. We are all uniquely predictable You ever watch enough of a certain kind of show, where you sorta just know everything that is going to happen? Same music, same tones, same characters, same story arcs but with a little twist? The show isn’t “real”, but the people that created that show are. Indirectly, everyone is a people watcher, and we all adapt to the patterns we are exposed to, even if we cannot always articulate it this way. Now the same “logic” or patterns that exist in watching CSI till its not fun anymore, also exist with businesses. We tend to forget that these big companies are just massive structures of political powerhouses that make money and serve their own societies. Like our own governments and the people they serve, they are also predictable. And like some of our own politicians, they sorta don’t always see or understand why people see them as “evil”. The reality is, and I HATE saying this because I'm more “consumer” than “Entrepreneur, Developer, Engineer, AI, Creepy Data Weirdo”, but that topic of “Evil” is relative to the person acting on it. We see some of the most amazing villains in film this way too, where we understand so much, that evil becomes more human and some point in the story everything changes. George Lucas factually had this powerful grasp of the patterns of human nature, but I think he sorta lost that connection when he started talking about the science of the force. Woah, George, we didn't need any of that reality in our not quite real sci-fi flick. Rise of the Machines We have factually always built more and more machines overtime to negate the use of physical or mental energy to make our lives easier. Even migrating into calculators and computers, its less that someone could NOT do those things, its that it would take a single person too much time, or have their own brains trained in REALLY messed up ways, or take even more time to properly break up the work to have others help them in a productive way. This is what the “weirdos” did, those weirdos that eventually would change the world, or at least become your boss. What a calculator does, is factually what Machine Learning does in Data Science, it's simply a different abstraction and business use case. It’s Math with more politics, more intuition, and things moving so fast you don't even know if Data Science is doing what it's supposed to be doing. That said, they are doing their best. The topics here are when we call something “AI” what most of us mean is “Machine Driven Calculation”, and/or a location in which decisions can be made in a digital environment. In Data Science we are building “Data Science Platforms” and in AI we are building “AI platforms”. In my industry, the “Game Industry” we are fighting it tooth and nail saying that AI can never replace creativity! Goodness, it is gonna hit my industry hard when they realize how close it is to level the playing field. Nvidia is getting close to ensuring that one. As with many things, they are solving the same issues, with the same pains, from different perspectives. The fact of the matter is, if you make decisions, you have access to data, you understand the patterns of various practices, you can either be empowered by AI or, inevitably replaced by it. It started with Factories, it's happening with Logistics (both human and goods) and “creativity” and “business intuition” is the next topic to be aware of. It's well on its way now, even if you try and ignore it. It’s Right Under Your Nose Today AI is a topic owned primarily by “Computer Science” and as engineers like to do, from Databases to AI, we consume ALL the things. We are more in line with Physicists than we are with practical developers if you give us a bit of flexibility. We can’t even help it. We were programmed this way almost out of the womb. But many of us become cold weirdos for a reason and when brilliant computer scientists start becoming philosophers, we are getting a bit too close to crazy. There should be some concern and I'm walking that line too (send help!). I recently had an exchange with a BRILLIANT and well-respected mind in AI leading a multimillion-dollar company in the efforts of AI. If you’re reading this its maybe not you, I talk to a lot of you, you already know I'm a chatterbox. Or it is you and I'm trying to salvage our relationship (❤ U). But I asked about how they handled their Data Architecture and Database and it was brushed off as if I was a lunatic. To be fair, that's a relatively true statement. Different article. But I also was well aware that because I don’t present my “Academic Background” my dialog will be subject to the bias of that individual's perspective. The reason that the topic is important, is because training AI is about the “Frequency” amount of data consumed and “Context”, as in what information is being used to train “AI” (or a person) towards an output. Inevitably if you build with your own bias and rely on “PhDs” you’ll overlook the context of what you're doing and the research that was done decades prior to doing, almost literally, the same things Academia has done before. The reality of this is, actual “AI” in so many ways is quite possible today, in many ways, it was done during the first boom in the 80s, we just forgot. Academia makes big money forgetting old research and repackaging it. You Are What you Eat If you cannot define intelligence and you have your own views and approach to building AI using machines and contextual data, your output, the thing you create, will inevitably just be an extension of you. Just like your art is an extension of you, or your business or culture is the same. So if you go at it training massive simulation data and ignore foundational topics of how brains work, which we determined literally decades ago, you're gonna end up making something as weird as you are. We do not train people by smashing all the things into them without there being some divides in actually understanding people. Because our brains are quite literally math, pattern consuming machines, subject to limited capabilities (cognitive load), we will create what we set out to when it comes to our machines. But this is the exact topic as to why AI today is not AI as you see it. More than likely, the more “Unicorn” an AI group is, the less likely it's doing AI. The logical irony being, Unicorns just aren’t real. AI platforms today are focussed on “Centralizing Data” or “Automating Machines”, both of them will call themselves AI or Data Science platform. The amount of funding they get has to do with who they know, what success they had, and how philosophical a group can be about selling the fantasy of it. The catch is, you cannot scale this way, because for an AI platform to actually work, an existing organization needs to be willing to release control of pretty much all of its data. And if you do that, its a breach of trust, security, people will lose jobs, and it's going to make a huge mess of an existing reporting structure. Not Trusting AI is a Good Thing, Sorta Today we have a lot of brilliant minds exploring AI for a variety of topics. Narrative, Business, Language, Bots, Logistics. If an Organic brain can determine a pattern and the term AI got them overly excited, they are gonna get all the money for it. Sad facts are, its characteristically more likely someone that currently DOESN’T trust AI, or doesn't want to understand it, that is best suited to use it. This happens because they have enough experience and exposure for their own minds to have been trained with powerful “Intuition”. “Cognitively” speaking, their own brains have consumed enough historical data and see enough patterns in current markets (and also dislike socializing enough) that their minds are as effective as Machines in many ways, they simply deal with their own overloads with creativity that makes it harder for some to work with. This shows in Depression, Anxiety, Drinking, or whatever output an individual has to make up for validation or connection. What those individuals do not realize, is that the way AI thinks is effectively closer to the way they think. If those minds could find a way to understand AI if a Computer Scientist could bypass their own theories of “Compute” or “AI”, some really amazing things would happen. We aren’t culturally there yet, but it should happen soon. We have big issues with separation when it comes to creativity, or business, or academic research. We all effectively have similar goals, we are just REALLY bad at communicating them in our respective languages. AI for ALL the things AI in practice is actually quite simple, but the approach we take today is due to “Societal Hubris”. It's very difficult to be challenged and communication is sensitive. In practice, linking references and articles and math on this post would spark a slew of debate and dialog to disprove decades of theories recreated and siloed. Or telling someone who just raised money on a billion-dollar valuation that they aren’t sneaky, they are just repackaging the same “Machines” with different languages and calling out how it won't work how they sell it also ain't the best way to make friends. But here is the thing. If you dig deep enough and you bypass your bias on what languages you use, what platforms, how to train Data, what Data Science is, etc, etc, its a fairly easy thing to figure out. The thing that makes it REALLY difficult, is it takes a genuine desire of “Cultural Collaboration” to actually build an AI company from the ground up. And its this reason why I also feel pretty strongly that the big companies today, Unicorn, Public Company, Investment Groups, etc are not likely going to be the company that cracks AI, at least not in the way they are selling it. Mostly, because we cracked it years ago and the very nature of “Innovation” is when we take a few old ideas and figure out how to bypass cultural barriers to make something new which was actually older than we realize. I still chuckle when someone working with Neural Networks gets excited that they realized it was like a brain, then publish a paper I know I saw a decade ago. Or when we look at an algorithm in one field, which existed in another, then someone called it “Machine Learning” and there is a slew of high fives forgetting about the nerds that did it first. Fun facts, Academia is more “Bro Culture” than it will admit today. And your idea of a Nerd is maybe WAY off track. Thanks, Pop Culture! Skynet vs. Jarvis I don’t think we give people enough credit, there is something mathematically inevitable about our capacity to solve problems and have society keep things in check. No matter how much a government watches, how off-balance a product is, or how inspiring a mind can be, social intelligence will always prevail. Sometimes “bad” things happen to allow better things to occur and failure forces change. What I can tell you now, after exposure to a LOT of “AI”, “Data Science” and “Data” companies, is their own objectives in leadership will inevitably result in something that is a reflection of who they are and who they surround themselves with. If you have a CEO that sells hype, with Investors that fund hype, or a public market that responds to emotions, they will keep themselves in the balance of their own motivations and this impacts EVERY person below them, systemically. The reality of this is, in order to understand AI, you factually, Mathematically, Scientifically, and Culturally need to understand people. And if you're too busy controlling AI today, you're probably not going to be creating AI for tomorrow. Because ultimately, society wants Jarvis, and even if they don’t know it, their actions will systemically assure Skynet doesn't happen. And if you’re a brilliant mind who is afraid of the end of times, you don't have enough understanding of the people you serve. You should spend more time with them. Unless we are already in the Matrix. Be nice to hackers.",
        "image": "https://miro.medium.com/v2/da:true/resize:fit:1200/0*bDFlV2hKht9_Ms9R",
        "favicon": "https://miro.medium.com/v2/resize:fill:256:256/1*VzTUkfeGymHP4Bvav-T-lA.png"
      }
    ],
    "effectiveFilters": {
      "includeDomains": [],
      "excludeDomains": [],
      "includeText": [],
      "excludeText": [],
      "urls": []
    },
    "costDollars": {
      "total": 0.015,
      "search": {
        "neural": 0.005
      },
      "contents": {
        "text": 0.01
      }
    }
  }
}